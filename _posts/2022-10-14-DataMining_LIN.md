---
title: 데이터마이닝 제 2,3강 작성중[KNU 2022-2]
categories:
  - KNU
tags:
  - DataMining
toc: true
---

# 👨‍💻🏫KNU 2022-2 SW & media 데이터마이닝 필기노트 2,3

**Regression Model**

# 1. Linear Regression

> $Y=ax+b$ <br>
> 모든 데이터를 직선으로 표현할 수 있다는 가정하에, 직선의 기울기와 절편을 찾는 것이 목표이다.<br>
> a, b는 회귀계수이며 X, Y는 실제 관측된 과거데이터 값이다.<br>
> a,b는 잔차(오차)의 합을 최소화하는 직선을 만든다(최소자승법) <br>

 총 n개의 객체중에 i번째 객체에대한 연속형 목표변수의 값을 $Y_i$입력변수의 값을  $X_{1i},X_{2i},...,X_{pi}$라고 하면 선형 회귀모형은 다음과 같이 정의된다.

$$Y_i = \beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + ... + \beta_pX_{pi} + \epsilon_i, i=1,...n$$

- $\beta_p$ : 회귀모수 또는 회귀계수로서 알려지지 않은 상수를 의미함
- $\epsilon_i$ : $Y_i$의 근사에서 오차를 의미함
- 각 객체는 서로 독립적이며 평균이 0이고 일정한 분산을 가진 정규분포의 형태를 가진다고 가정한다

## 1.1. 회귀모수의 추정

입력변수X와 목표변수 Y의 산점도에서 보듯이 각 관측치로부터 회귀직선까지의 수직거리 제곱을 최소화하는 $\beta$를 찾는 것이 목표이다. 방법으로는 최소제곱추정법(Least Square Estimation;LSE)을 사용한다.

$\sum_{i=1}^n \epsilon_{i}^2= \sum_{i=1}^n(Y_i-\beta_0-\beta_1X_{1i}-\beta_2X_{2i}-...-\beta_pX_{p i})^2$ 인 오차의 제곱합일때 이를 이용한 최소제곱 회귀곡선(Least Square Regression Line)은 다음과 같다.

$$\hat{Y_i} = \hat{\beta_0} + \hat{\beta_1}X_{1i} + \hat{\beta_2}X_{2i} + ... + \hat{\beta_p}X_{pi}, i=1,...,n$$

## 1.2. 회귀계수의 해석

- $\beta_i$ : 다른 입력변수를 보정한 후 Y에 대한 X의 기여도로 해석할 수 있다.
- 양수인경우 X,Y의 관계가 양의 상관관계(비례)를 가지고 음수인 경우 음의 상관관계(반비례)를 가진다.

## 1.3. 입력변수의 중요도

> 계수(변수)의 중요도는 t검정을 통해 알 수 있다. t검정은 다음과 같이 정의된다.

$$t_j = \frac{\hat{\beta_j}}{SE (\hat{\beta_j})}\\SE (\hat{\beta_j})는\; j번째\;회귀계수의\;추정치\; \hat{\beta}_I 의\; 표준편차 \\|t|가\; 클수록\; 영향력이있다.$$

## 1.4. 회귀모형의 적합도

### 1.4.1. F

> 모든계수에 대해서는 F검정을 수행, 모형 전체에 대하여는 ANOVA를 수행한다.
> 
- 모형의 상수항 $\beta_0$을 제외한 모든 회귀게수가 0인지를 검정하는 것이다. 이를 위해 F검정을 사용한다.
- 회귀직선에 의해 평균적으로 설명 가능한 부분(MSR)을 설명 불가한 부분(MSE)으로 나눈 값이다.
- F값이 작아 P값이 크면 입력변수가 목표변수에 대한 설명력이 낮다는 것을 의미한다.
- F값이 크면 P값이 작아 입력변수들 중 일부는 목표변수에 대한 설명력이 높다는 것을 의미한다.
  
$$F = \frac{MSR}{MSE}= \frac{SSR/p}{SSE/(n-p-1)}=\frac{\sum_{i=1}^n(\hat{Y_i}-\bar{Y})^2/p}{\sum_{i=1}^n(Y_i-\hat{Y_i})^2/(n-p-1)}$$

### 1.4.2. R($R^2$)

모형의 적합도를 결정계수($R^2$)라고 한다. 이는 설명 가능한 부분을 변동의 총합으로 나눈 값으로 0~1의 값이다. 결정계수는 다음과 같이 정의된다.

$$R^2 = \frac{SSR}{SST} = 1-\frac{SSE}{SST}, 0<R^2<1$$

변수의 수인 p가 증가하면 결정계수는 증가한다. 따라서 이런 경우 수정된 결정계수를 사용한다($R_{adjust}^2$).

입력변수의 수가 적은 다른 모형들을 비교 평가하는 기준으로 AIC(Akaike Information Criterion)도 사용된다. AIC는 다음과 같이 정의된다.

$$AIC = -n log(SSE/n) + 2p$$

## 1.5. 회귀모형의 예측

주어진 데이터에 기반하려 회귀식을 얻었을때, 임의의 객체 $i^*$에 대해 관측한 입력 변수의 값 $x_i^*,...,x_p^*$을 그 회귀식에 대입하여 목표 변수의 예측값 $\hat{Y_i^*}$을 구할 수 있다

$$\hat{Y_i^*} = \hat{\beta_0} + \hat{\beta_1}x_1i^* + ... + \hat{\beta_p}x_{pi^*}$$

## 1.6. 회귀모형 예측력의 검증

목표변수가 연속형인 경우 모형의 예측력 척도로서 MSE(Mean Squared Error)를 사용한다. MSE는 다음과 같이 정의된다.

$$MSE = \frac{1}{n}\sum_{i=1}^n(Y_i-\hat{Y_i})^2/n$$

## 1.7. 다항회귀분석

> X가 1차원이 아닌 경우 다항회귀분석을 사용한다.

## 1.8. 다중회귀분석

><br>  항이 여러개인 경우 다중회귀분석을 사용한다. 이때 비선형적인 결과의 양상을 보이게된다
><br>  종속변수와 2개 이상의 독립변수로 선형관계를 예측한다.
><br> EX: 
><br>  $ Y = 68.223 + 0.041*도시인구비율+1.1683*GDP $
><br> 도시인구비율이 1% 증가하면 Y는 0.041 증가한다. GDP와는 별개로 독립적인 특성을 가진다,  각 변수는 측정 단위가 다르기에 회귀계수는 변수의 설명력과 큰 관련이 없다

# 2. Logistic Regression

목표변수가 두개의 범주를 가진 이항형인 경우, 선형 회귀모형을 적용하면 0 또는 1과 다른 예측값을 얻거나 범위를 벗어난 값을 얻게 된다. 이런 경우에는 로지스틱 회귀모형을 사용한다. 이는 목표변수의 값이 1인 확률의 로짓변환과 입력변수들의 선형 함수 관계로 나타내는 모형인 것이다.

> 대표적인 비선형 모형이다.

## 2.1. 모형의 정의

> 이항형 목표변수 값을 $Y_i$라고 하면 $Y_i$의 값은 0 또는 1이다. 목표변수가 1을 가질 확률을 ${\pi = Pr(Y_i = 1)}$, $절편=b_0, 기울기 b_i$ 로 정의할 때 로지스틱회귀모형은 다음과 같다.

$$Y=\beta_0 + \beta_1X_{1i} + \beta_2X_{2i} + ... + \beta_pX_{p} , i=1,...n$$
$$logit({p})_i = \frac{exp(Y)}{1+exp(Y)}, i=1,...n$$

X가 입력변수의 값들이고 이항형목표변수는 이항분포를 따른다고 가정할때 다음과 같이 표기 가능

> $$log(\frac{p_i}{1-p_i})=Y$$

- 이때 $\beta_0, \beta_1, ..., \beta_p$는 회귀모수(계수)로 추정의 대상이 된다.

- $\pi$와 X의 관계는 로지스틱반응함수(Logistic Response Function)라고 한다 S자 형태의 그래프를 가지는 성장곡선의 형태를 가질 수 있다.

> 성공실패의 비$\frac{p}{1-p}$를 odds Ratio라고 하고 이에 로그를 취한것을 로짓변환이라고 부른다.

## 2.2. 회귀 모수의 추정

- 데이터의 확률함수를 $\beta$에 대한 함수로 취급한 것을 우도함수(likelihood function) $L(\beta)$라고 하고  이가 최대가 될때 모수의 추정값인 최도우도추정치$\hat{\beta}$를 통한 `최대우도추정법`에 의해 추정된다. 이때 우도함수를 최대화 하는 모수의 추정값은 뉴턴-랩슨, 피셔스코링에 의해 반복 계산을 통해 구할 수 있다.

## 2.3. 회귀계수의 해석

- $\beta_j$ : 다른 입력변수를 보정한 후 $log(\frac{\pi}{1-\pi})=Y=1$ 에 미치는 $X_j$의 효과

## 1.3. 입력변수의 중요도

중요도는 z검정을 통해 알 수 있다. z검정은 다음과 같이 정의된다.

j번째 입력 변수에 $X_j$ 대한 z값은 다음과 같다.

$$z_j = \frac{\hat{\beta_j}}{se(\hat{\beta_j})}\\SE (\hat{\beta_j})는\; j번째\;회귀계수의\;추정치$$

## 2.4. 회귀모형의 적합도

### 2.4.1. deviance

$$deviance = -2(log(L_M)-log(L_S))$$

- 어떤 모형의 최대로그우도($L_M$)에서 포화모형($L_S$)의 최대로그우도를 빼 -2를 곱한 값을 deviance라고 한다.
- 포화모형은 각 관측에 모수 하나씩 사용하여 완벽한 모형을 의미한다.
- 이탈도가 큰경우 포화모형보다 더 나은 모형이라고 할 수 없다.

### 2.4.2. AIC

$$AIC = -2log(L_M) + 2p$$

- p는 모수의 수를 의미한다.
- AIC는 모수나 입력변수의 수에 따라 deviance가 달라지는 것을 보완한 것으로 여러 후보 모형중 가장 작은 AIC값을 가지는 모형을 선택한다.

## 2.5. 회귀모형의 예측

임의의 객체 i^*에 대해 관측한 입력변수의 값 $X_{i^*}...X_{pi^*}$를 로지스티 회귀모형에 대입해 성공 확률 $\pi_{i^*}=Pr(Y_{i^*}=1)$의 예측값 $\hat{\pi}_{i^*}$를 구한다.

$$\hat{Y_i^*} = \hat{\beta_0} + \hat{\beta_1}x_1i^* + ... + \hat{\beta_p}x_{pi^*}$$

$$\hat{\pi}_{i^*}=\frac{exp(\hat{Y_i^*})}{1+exp(\hat{Y_i^*})}$$

예측값 $\hat{\pi}_{i^*}$가 0.5보다 크면 $Y_{i^*}=1$로 예측하고, 0.5보다 작으면 $Y_{i^*}=0$으로 예측한다. 이때 크고 작음을 판단하는 기준($\pi_0$)은 보통 0.5이나 적용분야에 따라 다르게 설정한다.

## 2.6. 회귀모형 예측력의 검증

- 목표변수가 이항형인 경우 관측된 목표변수 값과 예측값에 기반하여 정오분류표를 만들어 예측력을 검증한다.
- 예측력의 측도로는 민감도와 특이도가 있다. 민감도는 실제로 1 중에서 1을 예측한 비율이고, 특이도는 실제로 0 중에서 0을 예측한 비율이다.
- 예측 정확도는 민감도와 특이도의 가중평균
- 오분류율은 1-예측정확도이다.

$$민감도 = Pr(\hat{Y}=1|Y=1) = n_{11}/n_{1+}$$
$$특이도 = Pr(\hat{Y}=0|Y=0) = n_{00}/n_{0+}$$
$$예측정확도 = Pr(\hat{Y}=1|Y=1) + Pr(\hat Y=0|Y=0)= (n_{11}+n_{00})/n$$
$$오분류율 = Pr(\hat{Y} \neq1|Y=1) + Pr(\hat Y\neq0|Y=0)= (n_{10}+n_{01})/n$$
$$가중평균 = \frac{민감도*특이도}{민감도+특이도}$$

### 2.6.1. ROC 곡선

> 여러 가능한 임계치에 대해 민감도와 특이도를 계산하여 ROC 곡선을 그린다. ROC 곡선은 민감도와 특이도의 관계를 나타내는 곡선으로, 민감도와 특이도가 높을수록 예측력이 좋다고 말 할 수있기때문에 좌상단에 가까울수록 곡선 아래 면적이 커진다. 이 면적을 AUC(Area Under the Curve)라고 한다. AUC는 0.5에서 1.0 사이의 값을 가지며, 1에 가까울수록 예측력이 높다고 할 수 있다.

# 3. Categorical Variable Processing

-입력변수가 범주형인 경우레는 가변수(dummy variable)를 만들어 회귀모형에 적용한다.

- 입력변수 X가 a,b,c 3가지 범주를 가질때 가변수는 다음과 같이 만든다.

$$X' = \begin{cases} 1 & X=a \\ 0 & X \neq a \end{cases}, X'' = \begin{cases} 1 & X=b \\ 0 & X \neq b \end{cases}$$

<br>범주 a를 가질때 $X' = 1, X'' = 0$
<br>범주 b를 가질때 $X' = 0, X'' = 1$
<br>범주 c를 가질때 $X' = 0, X'' = 0$

# 4. Variable Selection for Model Building

> 다중회귀분석에서는 다른 변수에 의해 영햐을 받으므로 변수 선택이 중요하다.
> 다중공선성(독립변수간에 강한 선형관계)가 있으면 다중회귀 분석의 결과는 주의가 필요하다.

>변수선택: 서로 확실히 연관성(다중공선성)이 없는 것을 고르는것
><br>왜? Y에기여도가 좋은 X을 선정하기 위하여

>  다중 공선성 진단법
> 
> - 공차한계 0.1이하
> - VIF가10 이상인 경우

- 모형은 데이터를 잘 설명 할 수 있을만큼 복잡성을 띄고있어야 하며, overfitting되지 않아야 한다. 이를 위해 변수 선택을 통해 모형을 단순화 시키는 것이 필요하다.

- 입력변수가 많을 경우, 유지가 비효율적이며 모형의 예측력이 떨어질 수 있다.

- 중요 변수가 제거될 경우 모형의 편향이 커지고, 모형의 예측력이 떨어질 수 있다.

모형구축을 위한 변수선택 방법은 다음과 같다.

## 4.1. Backward Elimination

모든 변수를 포함한 모형을 만든 후, 가장 유의하지 않은 변수를 하나씩 제거하는 방법이다.

## 4.2. Forward Selection

상수항을 포함한 모형을 만든 후, 가장 유의한 변수를 하나씩 추가하며 남은 변수가 유일하자 않을때까지 진행

## 4.3. Stepwise Selection

전진선택법과 후진소거법을 결합한 방법으로, 전진선택법으로 모형을 만든 후, 유의하지 않은 변수를 하나씩 제거하는 방법이다.

# 5. 회귀식의 설명력

- 회귀식의 설명력은 P, R^2, Z등으로 설명 가능하다. 이러한 값을 설명게수라고 한다.
- Pearson 상관계수의 절대값이 1에 가까울수록 강한 설명 관계를 가진다고 할 수 있다.
  
- 각 계수마다 검정: t-test
- 모든 계수 검정: F-test
- 문자형 계수 검정: chi-square test(카이제곱검정)

다중회귀분석에서는 다른 변수에 의해 영향을 받으므로 변수의 선택이 중요핟하

-> 다중공선성(특정변숙수 )