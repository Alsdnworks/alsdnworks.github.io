---
title: ë¬¸ì œí•´ê²°í”„ë¡œê·¸ë˜ë° ìì „ê±° ìˆ˜ìš” ì˜ˆì¸¡[KNU 2022-2][kaggle]
categories:
  - KNU
tags:
  - ML
toc: true
---

# ğŸ‘¨â€ğŸ’»ğŸ«KNU 2022-2 SW & media ë¬¸ì œí•´ê²°í”„ë¡œê·¸ë˜ë° ìì „ê±° ìˆ˜ìš” ì˜ˆì¸¡

## 0.  Working Pipeline

| ìˆœì„œ | ì´ë²ˆ ê³¼ì œì—ì„œëŠ”.. |
|-|-|
|ê²½ì§„ëŒ€íšŒì˜ ì´í•´| 1. ë¬¸ì œ ì´í•´ (ë°°ê²½, ëª©ì , ìœ í˜•ë“±) <br> 2. í‰ê°€ì§€í‘œ íŒŒì•…|
|íƒìƒ‰ì  ë°ì´í„° ì´í•´|1. ë°ì´í„° íŒŒì•…(feature aggregation) <br> 2. ë°ì´í„° ì‹œê°í™”(ê°ì¢… í”¼ì³ì˜ ë¶„í¬í™•ì¸)|
|ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸|1. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§(íŒŒìƒ í”¼ì²˜ ì¶”ê°€)<br> 2. í‰ê°€ì§€í‘œ ê³„ì‚°í•¨ìˆ˜ ì‘ì„± <br> 3. ëª¨ë¸ í›ˆë ¨ ë° ì„±ëŠ¥ ê²€ì¦ <br> 4. ê²°ê³¼ ì˜ˆì¸¡ ë° ì œì¶œ|
|ì„±ëŠ¥ ê°œì„ |1. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§ <br> 2. í•˜ì´í¼íŒŒë¦¬ë¯¸í„° ìµœì í™” <br> 3. ì„±ëŠ¥ ê²€ì¦ <br> 4. ê²°ê³¼ ì˜ˆì¸¡ ë° ì œì¶œ|

## 1. ë°ì´í„° íŒŒì•…(feature aggregation)

~~~python
# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %%
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

pd.set_option('display.max_columns', 100)
train.head()

#ì´í•˜ ì¶œë ¥
#feature    -    explanation
#datetime - Timestamp(1T)
#season - 1. Spring, 2. Summer, 3. Fall, 4. Winter
#holiday - 0. No holiday, 1. Holiday
#workingday - 0. If day is neither weekend nor holiday, 1. Otherwise
#weather - 1.Clear, 2.Few clouds, 3.Rainy(Snowy), 4.Heavy Rainy(Snowy)
#temp - Temperature in Celsius
#atemp - "Feels like" temperature in Celsius
#humidity - Relative humidity
#windspeed - Wind speed
#casual - Number of non-registered user rentals initiated
#registered - Number of registered user rentals initiated
#count - Number of total rentals

# %%
print(train.shape,test.shape)
# %%
train.drop_duplicates()
test.drop_duplicates()
print(train.shape,test.shape)
# %%
train.info() 

#train['datetime']ì„ ë¶„í• í•˜ì—¬ ì—°, ì›”, ì¼, ì‹œê°„, ë¶„, ì´ˆ, ìš”ì¼ë¡œ ë‚˜ëˆ„ì–´ ìƒˆë¡œìš´ featureë¡œ ì¶”ê°€
# %%
from datetime import datetime
import calendar
train['year'] = train['datetime'].apply(lambda x: datetime.strptime(x.split()[0], '%Y-%m-%d').year)
train['month'] = train['datetime'].apply(lambda x: datetime.strptime(x.split()[0], '%Y-%m-%d').month)
train['day'] = train['datetime'].apply(lambda x: datetime.strptime(x.split()[0], '%Y-%m-%d').day)
train['hour'] = train['datetime'].apply(lambda x: x.split()[1].split(':')[0])
train['minute'] = train['datetime'].apply(lambda x: x.split()[1].split(':')[1])
train['second'] = train['datetime'].apply(lambda x: x.split()[1].split(':')[2])
train['weekday'] = train['datetime'].apply(lambda x: calendar.day_name[datetime.strptime(x.split()[0], '%Y-%m-%d').weekday()])

#season, weatherë¥¼ ë¬¸ìí˜•ìœ¼ë¡œ ë³€í™˜
# %%
train['season'] = train['season'].map({1:'Spring', 2:'Summer', 3:'Fall', 4:'Winter'})
train['weather'] = train['weather'].map({1:'Clear', 2:'Few clouds', 3:'Rainy', 4:'Heavy Rainy'})
~~~

## 2. ë°ì´í„° ì‹œê°í™”

### 2.1. ê° featureì˜ ë¶„í¬ í™•ì¸

barplot, boxplot, jointplot, pointplotë¥¼ í™œìš©í•˜ì—¬ ê° featureì˜ ë¶„í¬ë¥¼ í™•ì¸í•œë‹¤.

~~~python
# %%
import seaborn as sns
import matplotlib as mpl
mpl.rc('font', family='Malgun Gothic')
mpl.rc('axes', titlesize=20)

# %%
train['count']=np.log(train['count'])# í¸ì¤‘ë˜ì–´ ìˆëŠ” count featureë¥¼ log ë³€í™˜

# %%
figure,axes = plt.subplots(nrows=3,ncols=2)
plt.tight_layout()
figure.set_size_inches(10,9)

sns.barplot(data=train, x='year', y='count', ax=axes[0][0])
sns.barplot(data=train, x='month', y='count', ax=axes[0][1])
sns.barplot(data=train, x='day', y='count', ax=axes[1][0])
sns.barplot(data=train, x='hour', y='count', ax=axes[1][1])
sns.barplot(data=train, x='minute', y='count', ax=axes[2][0])
sns.barplot(data=train, x='second', y='count', ax=axes[2][1])

axes[0][0].set(ylabel='count', title='ì—°ë„ë³„ ëŒ€ì—¬ëŸ‰')
axes[0][1].set(xlabel='month', title='ì›”ë³„ ëŒ€ì—¬ëŸ‰')
axes[1][0].set(xlabel='day', title='ì¼ë³„ ëŒ€ì—¬ëŸ‰')
axes[1][1].set(xlabel='hour', title='ì‹œê°„ë³„ ëŒ€ì—¬ëŸ‰')
axes[2][0].set(xlabel='minute', title='ë¶„ë³„ ëŒ€ì—¬ëŸ‰')#TimestampëŠ” ì‹œë‹¨ìœ„ ë°ì´í„°ë¡œ ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì´ ë‚˜ì˜¨ë‹¤->ì œê±°ëŒ€ìƒ
axes[2][1].set(xlabel='second', title='ì´ˆë³„ ëŒ€ì—¬ëŸ‰')#TimestampëŠ” ì‹œë‹¨ìœ„ ë°ì´í„°ë¡œ ìœ íš¨í•˜ì§€ ì•Šì€ ê°’ì´ ë‚˜ì˜¨ë‹¤->ì œê±°ëŒ€ìƒ

axes[1][0].tick_params(axis='x', rotation=90)
axes[1][1].tick_params(axis='x', rotation=90)

# %%
figure,axes = plt.subplots(nrows=2,ncols=2)
plt.tight_layout()
figure.set_size_inches(10,10)

sns.boxplot(data=train, x='season', y='count', ax=axes[0][0])
sns.boxplot(data=train, x='holiday', y='count', ax=axes[0][1])
sns.boxplot(data=train, x='workingday', y='count', ax=axes[1][0])
sns.boxplot(data=train, x='weather', y='count', ax=axes[1][1])

axes[0][0].set(ylabel='count', title='ê³„ì ˆë³„ ëŒ€ì—¬ëŸ‰')
axes[0][1].set(xlabel='holiday', title='íœ´ì¼ ì—¬ë¶€ì— ë”°ë¥¸ ëŒ€ì—¬ëŸ‰')
axes[1][0].set(xlabel='workingday', title='ê·¼ë¬´ì¼ ì—¬ë¶€ì— ë”°ë¥¸ ëŒ€ì—¬ëŸ‰')
axes[1][1].set(xlabel='weather', title='ë‚ ì”¨ì— ë”°ë¥¸ ëŒ€ì—¬ëŸ‰')#í­ìš°(ì„¤)ë°ì´í„° í•˜ë‚˜ë§Œ ì¡´ì¬->ì œê±°ëŒ€ìƒ
#ê³„ì ˆê³¼ ë‚ ì”¨ë°ì´í„°ëŠ” ì°¨ì´ê°€ ê±°ì˜ ì—†ë‹¤

axes[1][1].tick_params(axis='x', rotation=90)

# %%
figure,axes = plt.subplots(nrows=2,ncols=2)
plt.tight_layout()
figure.set_size_inches(10,10)

sns.jointplot(data=train, x='temp', y='count', kind='reg', line_kws={'color':'blue'},ax=axes[0][0])
sns.jointplot(data=train, x='atemp', y='count', kind='reg', line_kws={'color':'blue'},ax=axes[0][1])
sns.jointplot(data=train, x='humidity', y='count', kind='reg', line_kws={'color':'blue'},ax=axes[1][0])
sns.jointplot(data=train, x='windspeed', y='count', kind='reg', line_kws={'color':'blue'},ax=axes[1][1])

axes[0][0].set(xlabel='temp', title='ì˜¨ë„ë³„ ëŒ€ì—¬ëŸ‰')
axes[0][1].set(xlabel='atemp', title='ì²´ê°ì˜¨ë„ë³„ ëŒ€ì—¬ëŸ‰')
axes[1][0].set(xlabel='humidity', title='ìŠµë„ë³„ ëŒ€ì—¬ëŸ‰')
axes[1][1].set(xlabel='windspeed', title='í’ì†ë³„ ëŒ€ì—¬ëŸ‰')#í’ì†ì´ 0ì¸ ë°ì´í„°ê°€ ë§ê³  ìœ ì‚¬ê°’ì´ ì—†ìŒ 
#-> ì‹ ë¢°ì„±ê³¼ ìƒê´€ì„±ì´ ë–¨ì–´ì§„ë‹¤ -> ì œê±°ëŒ€ìƒ

# %%
plt.figure(figsize=(12,4))
sns.pointplot(data=train, x='hour', y='count', hue='weekday')#ê·¼ë¬´ì•Œì—ëŠ” ì¶œí‡´ê·¼ì‹œê°„ì— ëŒ€ì—¬ëŸ‰ì´ ë§ë‹¤
~~~

### 2.2. ìƒê´€ê´€ê³„ ë¶„ì„

- ì—°ì† ë³€ìˆ˜ë¡œ ì¸¡ì •ëœ ë‘ ë³€ìˆ˜ê°„ì˜ ì„ í˜•ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” ê¸°ë²•
- ìƒê´€ê³„ìˆ˜ëŠ” -1~1ì‚¬ì´ì˜ ê°’ì„ ê°€ì§€ë©°, 1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ì–‘ì˜ ìƒê´€ê´€ê³„, -1ì— ê°€ê¹Œìš¸ìˆ˜ë¡ ìŒì˜ ìƒê´€ê´€ê³„ë¥¼ ì˜ë¯¸
- í•œ ë³€ìˆ˜ê°€ ì¦ê°€í•  ë•Œ ë‹¤ë¥¸ ë³€ìˆ˜ë„ ì¦ê°€í•˜ëŠ” ê²½ìš° ì–‘ì˜ ìƒê´€ê´€ê³„, ë°˜ëŒ€ì˜ ê²½ìš° ìŒì˜ ìƒê´€ê´€ê³„
- í†µê³„í•™ì—ì„œì˜ í”¼ì–´ìŠ¨ ìƒê´€ê³„ìˆ˜ë¥¼ ì˜ë¯¸
- ë‘ ë³€ìˆ˜ê°„ì˜ ì„ í˜•ê´€ê³„ì— ì´ˆì 
  
1. ì„ í˜•ê´€ê³„ë¥¼ ê°–ëŠ”ê°€?
2. ì„ í˜•ê´€ê³„ë¥¼ ê°–ëŠ”ë‹¤ë©´ ì–´ëŠ ë°©í–¥ìœ¼ë¡œ ê°–ëŠ”ê°€?
3. ì„ í˜•ê´€ê³„ê°€ ìˆë‹¤ë©´ ì–¼ë§ˆë‚˜ ê°•í•œê°€?

$$r=\frac{\sum_{i=1}^{n}(x_i-\bar{x})(y_i-\bar{y})}{\sqrt{\sum_{i=1}^{n}(x_i-\bar{x})^2}\sqrt{\sum_{i=1}^{n}(y_i-\bar{y})^2}}$$

~~~ python
# %%
corrMatt = train[['temp','atemp','humidity','windspeed','count']]
fig, ax = plt.subplots(figsize=(10,10))
fig.set_size_inches(10,10)
sns.heatmap(corrMatt.corr(), annot=True, linewidths=.3, ax=ax)
ax.set_title('í”¼ì³ê°„ ìƒê´€ê´€ê³„ ë¶„ì„')
~~~

## 3. feature engineering

train, test ë°ì´í„°ë¥¼ í•©ì³ì„œ ì „ì²˜ë¦¬ë¥¼ ì§„í–‰í•œë‹¤.

~~~ python
all_data = pd.concat([train, test],ignore_index=True)
~~~

ë°ì´í„°ì—ì„œ ìƒˆë¡œìš´ í”¼ì³ë¥¼ ì¶”ì¶œí•˜ê³  ì‚¬ìš©ë˜ëŠ” featureë§Œ ì¶”ì¶œí•œë‹¤.

~~~ python

# %%
from datetime import datetime
import calendar

#train['datetime']ì„ ë¶„í• í•˜ì—¬ ì—°, ì›”, ì‹œê°„, ìš”ì¼ë¡œ ë‚˜ëˆ„ì–´ ìƒˆë¡œìš´ featureë¡œ ì¶”ê°€
#ì¼ì€ ìš”ì¼ê³¼ ê°™ì€ ë°ì´í„°ì´ë¯€ë¡œ ë¯¸ì‚¬ìš©
#ë¶„, ì´ˆëŠ” ëª¨ë‘ 0ì´ë¯€ë¡œ ë¯¸ì‚¬ìš©
#weatherê°€ 4ì¸ ë°ì´í„°ëŠ” ì œê±°
all_data['year'] = all_data['datetime'].apply(lambda x: datetime.strptime(x.split()[0], '%Y-%m-%d').year)
all_data['month'] = all_data['datetime'].apply(lambda x: datetime.strptime(x.split()[0], '%Y-%m-%d').month)
all_data['hour'] = all_data['datetime'].apply(lambda x: x.split()[1].split(':')[0])
all_data['weekday'] = all_data['datetime'].apply(lambda x: calendar.day_name[datetime.strptime(x.split()[0], '%Y-%m-%d').weekday()])
all_data['weather']=all_data[all_data['weather']!=4]
#í•„ìš”ì—†ëŠ” ë‚˜ë¨¸ì§€ feature ì œê±°
all_data.drop(['datetime','casual','registered','windspeed'], axis=1, inplace=True)
#ì¤‘ë³µì„±ì„ ë„ìš°ëŠ” ì›”ê³¼ ê³„ì ˆì¤‘ í•˜ë‚˜ë¥¼ ì œê±°
 del all_data['month']
~~~

í›ˆë ¨, ì˜ˆì¸¡ ë°ì´í„°ë¥¼ ë‹¤ì‹œ ë¶„ë¦¬í•œë‹¤. ê¸°ì¡´ train 8708ì¤‘ ì œê±°ëœ í–‰ì€ í•˜ë‚˜ì´ë¯€ë¡œ 8707ê°œì˜ í–‰ì„ ê°€ì§„ë‹¤.

~~~ python
X_train= all_data[:8707]
X_test= all_data[8707:]
y_train= X_train[['count']]
y_test= X_test[['count']]
del X_train['count']
del X_test['count']
~~~

## 4. ì„ í˜• ëª¨ë¸ë§

### 4.1 ì„ í˜•íšŒê·€

#### 4.1.1 í‰ê°€ì§€í‘œ

ë³¸ ë¬¸ì œëŠ” RMSLEë¥¼ ì‚¬ìš©í•˜ì—¬ í‰ê°€í•˜ì˜€ë‹¤. RMSLEëŠ” MSLEì— ë¡œê·¸ë¥¼ ì·¨í•œ ê°’ì´ë‹¤.

~~~ python

##########################################
#ê°„ëµí•œ ì„¤ëª…
from sklearn.metrics import mean_squared_log_error
def RMSLE(y_true, y_pred,convertExp):
    if convertExp:
        y_predExp=np.exp(y_pred)
        y_trueExp=np.exp(y_true)
        msle=mean_squared_log_error(y_trueExp, y_predExp)
    else:
        msle=mean_squared_log_error(y_true, y_pred)
    return np.sqrt(msle)
##########################################
# ì‹¤ì œ ìˆ˜ì—…ì‹œê°„ì— ì‚¬ìš©í•œ ì½”ë“œ
def RMSLE(y_true, y_pred,convertExp):
    if convertExp:
        y_pred=np.exp(y_pred)
        y_true=np.exp(y_true)
    log_true=np.nan_to_num(np.log(y_true+1))
    log_pred=np.nan_to_num(np.log(y_pred+1))
    calc=np.sqrt(np.mean(np.power(log_true-log_pred,2)))
    return calc
~~~

#### 4.1.2. ëª¨ë¸ ì‘ì„±

y= ax+b í˜•íƒœì˜ ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•œë‹¤. ì„ í˜•íšŒê·€ ëª¨ë¸ì€ ë‹¤ìŒê³¼ ê°™ì€ íŠ¹ì§•ì„ ê°€ì§„ë‹¤.

$$\hat{y}=w_0x_0+w_1x_1+...+w_nx_n+b$$

|||
|-|-|
|$x_0 ... x_p$|ë°ì´í„°ì˜í”¼ì³, í”¼ì³ìˆ˜ëŠ” p+1|
|$w,b$|ëª¨ë¸ì´ í•™ìŠµí•˜ëŠ” íŒŒë¼ë¯¸í„°|
|$\hat{y}$|ì˜ˆì¸¡ìœ¼ë¡œ ì…ë ¥ íŠ¹ì„±ì— ê°€ì¤‘ì¹˜ë¥¼ ê³±í•´ì„œ ë”í•œ ê°€ì¤‘ì¹˜ì˜ í•©|

~~~ python
#ì„ í˜•íšŒê·€ ëª¨ë¸ì„ ì‚¬ìš©í•˜ì—¬ í•™ìŠµ
# %%
from sklearn.linear_model import LinearRegression
Lin_model=LinearRegression()

log_y_train=np.log(y_train)
log_y_test=np.log(y_test)
Lin_model.fit(X_train,log_y_train)

print(Lin_model.coef_)#ì„ í˜•íšŒê·€ ê²°ê³¼ì˜ ê³„ìˆ˜
print(Lin_model.intercept_)#ì„ í˜•íšŒê·€ ê²°ê³¼ì˜ ìƒìˆ˜í•­(ì ˆí¸)

print(Lin_model.predict(X_test))
print(Lin_model.predict(X_test))

#ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ í™•ì¸í•´ë³¸ë‹¤(train)
# %%
plt.figure(figsize=(10,5))
x=np.linspace(0,7)
y=x
lines=plt.plot(Lin_model.predict(X_train),log_y_train,'o',x,y,'k')
plt.setp(lines[0],markersize=5)
plt.setp(lines[1],linewidth=4,color='r')
plt.xlabel('Predicted')
plt.ylabel('Observed')
plt.show()

#ê²°ê³¼ë¥¼ ê·¸ë˜í”„ë¡œ í™•ì¸í•´ë³¸ë‹¤(test)
plt.figure(figsize=(10,5))
x=np.linspace(0,7)
y=x
lines=plt.plot(Lin_model.predict(X_test),log_y_test,'o',x,y,'k')
plt.setp(lines[0],markersize=5)
plt.setp(lines[1],linewidth=4,color='r')
plt.xlabel('Predicted')
plt.ylabel('Observed')
plt.show()
~~~

#### 4.1.3 ì„±ëŠ¥í‰ê°€

ìœ„ì—ì„œ ì‘ì„±í•œ RMSLEí•¨ìˆ˜ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ì„ í‰ê°€í•œë‹¤.

~~~python
# %%
print(RMSLE(log_y_train,Lin_model.predict(X_train),True))
print(RMSLE(log_y_test,Lin_model.predict(X_test),True))
~~~

 ì´í•˜ ê¸°ë§ê³ ì‚¬ ë²”ìœ„---------------------------------------------------------

### 4.2. ë¦¿ì§€ ì„ í˜• íšŒê·€(Ridge Regression)

- ì„ í˜•ëª¨ë¸ + ê°€ì¤‘ì¹˜ ìµœì†Œí™”
- ìµœì†Œì í•©ì  ì˜ˆì¸¡í•¨ìˆ˜
- w(ê°€ì¤‘ì¹˜): ê²°ê³¼ ì¶œë ¥ì— ë¯¸ì¹˜ëŠ” ì¤‘ìš”ë„ë¥¼ ì¡°ì ˆí•˜ëŠ” ë§¤ê°œë³€ìˆ˜
- ì„¤ëª…ë³€ìˆ˜ì˜ ìˆ˜ê°€ ì¦ê°€í•  ê²½ìš° ì„¤ëª…ë³€ìˆ˜ê°„ ë‹¤ì¤‘ê³µì„ ì„±ì´ ìƒê¸¸ ê°€ëŠ¥ì„±ì´ ì¡´ì¬í•˜ë©° ëª¨ë¸ì´ ì§€ë‚˜ì¹˜ê²Œ ë³µì¡í•´ì§„ë‹¤.
- íšŒê·€ê³„ìˆ˜ì˜ ì¶•ì†Œ(the shirinkage penalty) ë³´ë‹¤ ì ì€ ë³€ìˆ˜ì˜ ì‚¬ìš©ìœ¼ë¡œ ëª¨ë¸ì˜ ê°„ê²°í™”

#### 4.2.1. norm

- ë²¡í„°ì˜ í¬ê¸°ëŠ” ì›ì ì—ì„œ ë¶€í…… ë²¡í„° ì¢Œí‘œê¹Œì§€ì˜ ê±°ë¦¬(Magnitude)
  - p: normì˜ ì°¨ìˆ˜
  - n: ëŒ€ìƒ ë²¡í„°ì˜ ìš”ì†Œ ìˆ˜
- ê° ìš”ì†Œë³„ë¡œ ìš”ì†Œ ì ˆëŒ€ê°’ì„ pë²ˆ ê³±í•œ ê°’ì˜ í•©ì„ pì œê³±ê·¼ ê°’

$$L_p=(\sum_{i}^{n}|x_i|^p)^\frac{1}{p} ... L_2=\sqrt{\sum_{i}^{n}x_i^2}$$

#### 4.2.2. ê°€ì¤‘ì¹˜ ìµœì†Œí™”

- ê°€ì¤‘ì¹˜ì˜ ì ˆëŒ€ê°’ì„ ê°€ëŠ¥í•œ ì‘ë„ë¡ ê·œì œ/ ì œì•½(Regularization)í•˜ì—¬ ëª¨ë“  ì›ì†Œê°€ 0ì— ê°€ê¹ê²Œ ë§Œë“¤ì–´ ëª¨ë“  íŠ¹ì„±ì´ ì¶œë ¥ì— ì£¼ëŠ” ì˜í–¥ì„ ìµœì†Œí•œìœ¼ë¡œí•´ ê³¼ëŒ€ ì í•©ì´ ë˜ì§€ì•Šë„ë¡ ëª¨ë¸ì„ ê°•ì œë¡œ ì œí•œ

- ì¡°ì ˆëª¨ìˆ˜ $\lambda$ (the tuning parameter, Penalized Regression)ì‚¬ìš©: 

1. ì¡°ì ˆ ëª¨ìˆ˜ê°€ í´ìˆ˜ë¡ coefficientë¥¼ì¶•ì†Œ(shrinkage)
2. ê³¼ì í•©ì„ ë§‰ê¸°ìœ„í•´ ì”ì°¨ë¥¼ ê³„ì‚°í•˜ëŠ” costí•¨ìˆ˜ì— penaltyë¥¼ë¶€ì—¬
3. ìˆœìˆ˜í•œ ì˜¤ì°¨ê°’ì— ê³¼í•˜ê²Œ ë°˜ì‘í•˜ì§€ ì•Šë„ë¡í•˜ì—¬ ê³¼ì í•©ì„ ë°©ì§€
4. íšŒê·€ì¶”ì •ì¹˜ì™€ ê´€ë ¨ëœ ì˜í–¥ì„ ì¡°ì ˆ

|$\lambda \Rightarrow 0$|$\lambda \Rightarrow \infty$|
|:---:|:---:|
|penaltyí•­ì€ íš¨ê³¼ê°€ ì—†ë‹¤|the shrinkage penaltyì˜ ì˜í–¥ì´ ì»¤ì§|
|ìµœì†Œ ì œê³±ì˜ ì¶”ì •ì¹˜(ë³µì¡ë„ì˜ ì¦ê°€, ê³¼ëŒ€ì í•© ê°€ëŠ¥)|ê³„ìˆ˜ ì¶”ì •ì¹˜ëŠ” 0ì— ê°€ê¹Œì›Œì§(ë³µì¡ë„ ê°ì†Œ, ê³¼ì†Œì í•©ì˜ ê°€ëŠ¥)|

|$\lambda$ ê°€ í´ìˆ˜ë¡|$\lambda$ ê°€ ì‘ì„ìˆ˜ë¡|
|:--:|:--:|
|ê³„ìˆ˜ì˜ í¸ì°¨ê°€ ì¢ë‹¤|ê³„ìˆ˜ í¸ì°¨ê°€ í¬ë‹¤(ê³¼ëŒ€ì í•©)|
|ê³„ìˆ˜ë¥¼ 0ì— ë” ê°€ê¹ê²Œ|ê³„ìˆ˜ì— ê±°ì˜ ì œí•œì„ ë‘ì§€ ì•ŠëŠ”ë‹¤|
|í›ˆë ¨ë°ì´í„°ì˜ ì„±ëŠ¥ì´ ë‚˜ë¹ ì§„ë‹¤|ì„ í˜•ëª¨ë¸ì— ê·¼ì‚¬í•´ì§„ë‹¤|
|ì¼ë°˜í™”ì— ë„ì›€|..|

#### 4.2.3. ëª¨ë¸ì‘ì„±

~~~python
from sklearn.linear_model import Ridge
ridge = Ridge(alpha=1.0)#í˜ë„í‹°í•­ì˜ ê°€ì¤‘ì¹˜
ridge.fit(X_train, log_y_train)

print('ê°€ì¤‘ì¹˜: ', ridge.coef_)
print('ì ˆí¸: ', ridge.intercept_)

ridge_pred_train = ridge.predict(X_train)
ridge_pred_test = ridge.predict(X_test)

print('RMSLE train: ', rmsle(log_y_train, ridge_pred_train))
print('RMSLE: test', rmsle(log_y_test, ridge_pred_test))
~~~

### 4.3. ë¼ì˜(Lasso)íšŒê·€

- ê°€ì¤‘ì¹˜ì˜ ì˜í–¥ì„ ìµœì†Œí™”
  - ê°€ì¤‘ì¹˜ê°€ 0ì´ ë  ìˆ˜ìˆë‹¤
  - ê³„ìˆ˜ì˜ í¸ì°¨ë¥¼ ê²€ì†Œ
  - íŠ¹ì„±ì„ ì„ ë³„í•´ì„œ í•´ì„ë ¥ì„ ë³´ì¸ë‹¤
- loss+penalty
  - L1-norm

|$\lambda$ ê°€ í´ìˆ˜ë¡|$\lambda$ ê°€ ì‘ì„ìˆ˜ë¡|
|:--:|:--:|
|L1ë¹„ì¤‘ ì¦ê°€ë¡œ ê³¼ì†Œì í•©(ë³µì¡ë„ ê°ì†Œ)|ê³¼ëŒ€ì í•©ìœ¼ë¡œ ë³µì¡ë„ê°€ ì¦ê°€í•˜ì—¬ ì„ í˜•íšŒê·€ì™€ ìœ ì‚¬í•œ í˜•íƒœë¥¼ ë³´ì¸ë‹¤|
|íŒ¨ë„í‹°ì˜ íš¨ê³¼ì¦ëŒ€(ê°€ì¤‘ì¹˜ì˜ ê°ì†Œ)|..|

- $\lambda$ì—ë”°ë¼ì„œ ì‚¬ìš©ë˜ëŠ” íŠ¹ì„±ì˜ ê°œìˆ˜ë¥¼ ì¡°ì ˆê°€ëŠ¥í•˜ë‹¤.
- íŠ¹ì§• ì¤‘ ì¼ë¶€ê°€ ì¤‘ìš”í• ë•Œ ì‚¬ìš©
- ì™„ì „íˆ ì œì™¸ë˜ëŠ” íŠ¹ì§•ì´ ë°œìƒí•œë‹¤

#### 4.3.1. ëª¨ë¸ì‘ì„±

~~~python
# %%
from sklearn.linear_model import Lasso
lasso = Lasso(alpha=1.0)
lasso.fit(X_train, log_y_train)
print('ê°€ì¤‘ì¹˜: ', lasso.coef_)
print('ì ˆí¸: ', lasso.intercept_)

lasso_pred_train = lasso.predict(X_train)
lasso_pred_test = lasso.predict(X_test)

# ë°˜í™˜ í˜•íƒœreshape
lasso_pred_train = lasso_pred_train.reshape(len(lasso_pred_train), 1)
lasso_pred_test = lasso_pred_test.reshape(len(lasso_pred_test), 1)

print('RMSLE train: ', rmsle(log_y_train, lasso_pred_train))
print('RMSLE: test', rmsle(log_y_test, lasso_pred_test))
~~~

#### 4.3.2. ë¦¿ì§€, ë¼ì˜, ëŒë‹¤ ì •ë¦¬

-> ë¦¿ì§€ì™€ ë¼ì˜ íšŒê·€ëŠ” ì„ í˜• íšŒê·€ëª¨ë¸ë¡œ íŒ¨ë„í‹°ë¥¼ ì´ìš©í•˜ì—¬ ê³„ìˆ˜ì˜ ì˜í–¥ë ¥ì„ ì¡°ì ˆ
L1/L2íŒ¨ë„í‹°ë¥¼ ì´ìš©, $\lambda$ë¡œ ì˜í–¥ë ¥ ì¡°ì ˆ

|ë¦¿ì§€|ë¼ì˜|
|-|-|
|Wì˜ ëª¨ë“  ì›ì†Œê°€ 0ì— ê°€ê¹ê²Œ|wì´ 0 ì´ ë  ìˆ˜ ìˆë‹¤|
|ê³„ìˆ˜ì˜ í¸ì°¨ë¥¼ ê°‘ì†Œì‹œí‚¨ë‹¤ë‹¤|íŠ¹ì§•ì„ ì„ ë³„í•˜ì—¬ í•´ì„ë ¥ì„ ë³´ì¸ë‹¤|
|L2norm|L1norm|
|ì„¤ëª…ë³€ìˆ˜ê°€ ë¹„ìŠ·í•œ ì¤‘ìš”ë„ë¥¼ ê°€ì§„ë‹¤ë‹¤|ì„¤ëª…ë³€ìˆ˜ë“¤ì¤‘ ì¼ë¶€ê°€ ì¤‘ìš”í•˜ê²Œ ëœë‹¤|
|ëª¨ë“  íŠ¹ì§•ë“¤ì˜ ì˜í–¥ì—­ì´ ê°ì†Œ|ì™„ì „íˆ ì œì™¸ë˜ëŠ” íŠ¹ì§•|
|ì¼ë°˜ì ìœ¼ë¡œ ì„ í˜¸ë¨|ë¶„ì„ê³¼ ì´í•´ê°€ ì‰¬ìš´ ëª¨ë¸|

|$\lambda$ê°€ ì‘ìœ¼ë©´|$\lambda$ê°€ í¬ë©´|
|-|-|
|MSEë¹„ì¤‘ ì¦ê°€|L1/L2ê°€ ë¹„ì¤‘ ì¦ê°€|
|ê³¼ëŒ€ ì í•©|ê³¼ì†Œ ì í•©|
|ë³µì¡ë„ì˜ ì¦ê°€|ë³µì¡ë„ì˜ ê°ì†Œ|
|ì„ í˜•íšŒê·€ì™€ ìœ ì‚¬|íŒ¨ë„í‹° íš¨ê³¼ê°€ ì¦ê°€|
|ê³„ìˆ˜ í¸ì°¨ê°€ í¬ë‹¤|ê°€ì¤‘ì¹˜ê°€ ê°ì†Œ|
|ê³„ìˆ˜ì˜ ì œí•œì´ ì—†ë‹¤|ì¼ë°˜í™”ì— ë„ì›€|

## 5. ê²°ì •íŠ¸ë¦¬(Decision tree)

ë¶„ë¥˜/íšŒê·€ì— ì‚¬ìš©. ê²°ì •ì— ì´ë¥´ê¸°ìœ„í•´ ê°€ëŠ¥í•œí•œ ì ì€ Y/Nì§ˆë¬¸ ì‚¬ìš©

(ex: ë¶„ë¥˜ì˜ í˜•íƒœ)
![[32F3A479-5910-4CC4-912F-6BC0B11C3BA1.jpeg]]

- ë¶ˆí™•ì‹¤ì„±

> í•œ ë…¸ë“œì˜ ìƒ˜í”Œì´ ê°™ì€ í´ë˜ìŠ¤ì— ì†í•´ìˆë‹¤ë©´ ì´ ë…¸ë“œëŠ” ìˆœìˆ˜
> 0ì—ì„œ 1ì‚¬ì´ì˜ ê°’ì„ ì‚¬ìš©í•œë‹¤
> ê°’ì´ í´ìˆ˜ë¡ ìˆœë„ê°€ ê°ì†Œí•˜ê³  ë¶ˆí™•ì‹¤ì„±ì´ ì»¤ì§
> ëŒ€í‘œì ì¸ ê³„ì‚°ë²•ìœ¼ë¡œ ì§€ë‹ˆë¶ˆìˆœë„(ê³„ìˆ˜) ì‚¬ìš©  - $p_{i}^{2},k$: ië²ˆì§¸ ë…¸ë“œ ìƒ˜í”Œì¤‘ í´ë˜ìŠ¤ kì—ì†í•  í™•ë¥  $G_i = 1- \sum_{k=1}^{n}p_{i}^{2},k$

~~~python
# %%
from sklearn.tree import DecisionTreeRegressor, plot_tree

DT_model = DecisionTreeRegressor(max_depth=2)# ê¹Šì´ì— ë”°ë¼ ëª¨ë¸ì˜ ë³µì¡ë„ê°€ ë‹¬ë¼ì§
DT_model.fit(X_train, log_y_train)

DT_pred_train=RF_model.predict(X_train)
DT_pred_test=RF_model.predict(X_test)

print(RMSLE(log_y_train,DT_pred_train,True),RMSLE(log_y_test,DT_pred_test,)True)

#Treeë¥¼ ì‹œê°í™”
# %%
plot_tree(DT_model, feature_names=X_train.columns, filled=True)
~~~

### 5.1. ì•™ìƒë¸” ê¸°ë²•

ì—¬ëŸ¬ê°œì˜ ëª¨ë¸ì„ ì¡°í™”ë¡­ê²Œ í•™ìŠµì‹œì¼œ ë” ì •í™•í•œ ì˜ˆì¸¡ê°’ì„ ë„ì¶œí•˜ëŠ” ê¸°ë²•

- ë°°ê¹… ê¸°ë²•(**B**ootstrap **Agg**regation)
  - ìƒ˜í”Œì„ ì—¬ëŸ¬ê°œ ë½‘ì•„(Bootstrap) ê°ëª¨ë¸ì„ í•™ìŠµì‹œì¼œ ê²°ê³¼ë¬¼ì„ ì§‘ê³„(Aggregation)í•˜ëŠ” ë°©ë²•
- ë¶€ìŠ¤íŒ… ê¸°ë²•

|ì¥ì |ë‹¨ì |
|-|-|
|1. íšŒê·€ì™€ ë¶„ë¥˜ì—ì„œ ê°€ì¥ ë„ë¦¬ ì‚¬ìš©ë˜ëŠ” ì•Œê³ ë¦¬ì¦˜<br>2. ë›°ì–´ë‚œ ì„±ëŠ¥ê³¼ ë§¤ê°œë³€ìˆ˜ íŠœë‹ ë¶€ë‹´ì´ ì ìŒ(ê¸°ë³¸ê°’)<br>3. ë°ì´í„° ìŠ¤ì¼€ì¼ ì¡°ì • ë¶ˆí•„ìš”<br>4. í° ë°ì´í„°ì…‹ ì ìš© ê°€ëŠ¥|1. ë§ì€ íŠ¸ë¦¬ê°€ ìƒì„±ë˜ë¯€ë¡œ ìì„¸í•œ ë¶„ì„ì´ ì–´ë µë‹¤<br>2. íŠ¸ë¦¬ê°€ ê¹Šì–´ì§€ëŠ” ê²½í–¥ì„ ë³´ì„<br>3. ì°¨ì›ì´ í¬ê³  í¬ì†Œí•œ ë°ì´í„°ì— ì„±ëŠ¥ì´ ì¢‹ì§€ì•ŠìŒ(ì˜ˆ: í…ìŠ¤íŠ¸ ë°ì´í„°)<br>4. ì„ í˜•ëª¨ë¸ë³´ë‹¤ ë©”ëª¨ë¦¬ì‚¬ìš©ëŸ‰ì´ í¼<br>5. í›ˆë ¨ê³¼ ì˜ˆì¸¡ì´ ëŠë¦¼|

#### 5.1.1. ëœë¤ í¬ë ˆìŠ¤íŠ¸

ë°°ê¹…ì˜ ëŒ€í‘œì ì¸ ì•Œê³ ë¦¬ì¦˜. ì—¬ëŸ¬ê°œì˜ ê²°ì •íŠ¸ë¦¬ë¥¼ ë§Œë“¤ê³  ê° íŠ¸ë¦¬ì˜ ì˜ˆì¸¡ì„ ì§‘ê³„í•˜ì—¬ ìµœì¢… ì˜ˆì¸¡ì„ ë§Œë“œëŠ” ë°©ë²•

~~~python
# %%
from sklearn.ensemble import RandomForestRegressor
RF_model=RandomForestRegressor(n_estimators=10)#ìƒ˜í”Œì„ 10ê°œ ë½‘ì•„ì„œ 10ê°œì˜ íŠ¸ë¦¬ë¥¼ ë§Œë“¬
RF_model.fit(X_train,log_y_train)

RF_pred_train=RF_model.predict(X_train)
RF_pred_test=RF_model.predict(X_test)

RF_pred_train=RF_pred_train.reshape(len(RF_pred_train),1)
RF_pred_test=RF_pred_train.reshape(len(RF_pred_test),1)

print(RMSLE(log_y_train,RF_pred_train,True),RMSLE(log_y_test,RF_pred_test,)True)

#ì‹œê°í™”
# %%
feature_importance=pd.DataFrame(RF_model.feature_importances_,index=X_train.columns,columns=['importance'])
~~~

## 6. í•˜ì´í¼ íŒŒë¼ë¯¸í„°ì˜ ìµœì í™”

1. ê·¸ë¦¬ë“œì„œì¹˜: ì£¼ì–´ì§„ í•˜ì´í¼íŒŒë¦¬ë¯¸í„°ë¥¼ ëª¨ë‘ ìˆœíšŒí•˜ì—¬ ê°€ì¥ ì¢‹ì€ ì„œì€ì„ ë°œíœ˜í•œ ã„´ê°’ì„ ì°¾ëŠ” ë°©ë²•
2. ëœë¤ì„œì¹˜: í•˜ì´í¼íŒŒë¦¬ë¯¸í„°ë¥¼ ë¬´ì‘ìœ„ë¡œ íƒìƒ‰í•˜ì—¬ ê°€ì¥ ì¢‹ì€ ì„±ëŠ¥ì„ ë°œíœ˜í•˜ëŠ” ê°’ì„ ì°¾ëŠ” ë°©ë²•
3. ë² ì´ì§€ì•ˆ ìµœì í™”: ì‚¬ì „ ì •ë³´ë¥¼ ë°”íƒ•ìœ¼ë¡œ ìµœì ì˜ í•˜ì´í¼ íŒŒë¼ë¯¸í„° ê°’ì„ í™•ë¥ ì ìœ¼ë¡œ ì¶”ì •í•˜ë©° íƒìƒ‰í•˜ëŠ” ë°©ë²•

**ëœë¤í¬ë ˆìŠ¤íŠ¸ë¥¼ ëŒ€ìƒìœ¼ë¡œ ê·¸ë¦¬ë“œì„œì¹˜ ì‹¤ìŠµ**

~~~python
# %%
from sklearn.model_selection import GridSearchCV
from sklearn import metrics#í‰ê°€ì§€í‘œ

RMSLE_scorer=metrics.make_scorer(RMSLE,greater_is_better=False)#í‰ê°€ì§€í‘œëŠ” ì‘ì„ìˆ˜ë¡ ì´ë“
RF_params={
    'n_estimators':list(range(100,200,50)),
    'max_depth':list(range(5,10)),
    'n_estimators':list(range(50,300,50))
}

GS_RF_model=GridSearchCV(\
                                            estimator=RF_model,\
                                            param_grid=RF_params,\
                                            scoring=RMSLE_scorer,\
                                            cv=5\
                                            )
GS_RF_model.fit(X_train,log_y_train)


print(GS_RF_model.best_params_)
print(GS_RF_model.best_estimator_.predict(X_train))

print(RMSLE(log_y_train,GS_RF_model.best_estimator_.predict(X_train),True))
print(RMSLE(log_y_test,GS_RF_model.best_estimator_.predict(X_test),True))
~~~

## 7. êµì°¨ê²€ì¦

train->testì˜ ë‹¨ì : ëª¨ë¸ì˜ ê³¼ì í•©ì„ ë°©ì§€í•˜ê³  ëª¨ë¸ì˜ ì„±ëŠ¥ì„ ë¯¸ë¦¬ í™•ì¸í•˜ê¸° í˜ë“¤ë‹¤
ì´ë¥¼ ìœ„í•œ vaildationì´ ì¶”ê°€ë˜ì–´ì•¼í•œë‹¤

### 7.1. K-Fold Cross Vaildation

![[F3F29782-7418-4F52-B9A4-0CE4AFF18248.jpeg]]

~~~python
# %%
from sklearn.model_selection import KFold
data=np.array(range(10))
exam_folds=KFold(n_splits=5,shuffle=False) #ì…”í”Œì˜ ì—¬ë¶€ë¥¼ ê²°ì •
for train_index,test_index in exam_folds.split(data):
    print('train_index:',data[train_index],'test_index:',data[test_index])
~~~

#### 7.2.1. Stratified K-Fold Cross Vaildation

ì‚¬ìš©ëª©ì : ë¶ˆê· í˜•í•œ ë°ì´í„°ì…‹ì„ êµì°¨ê²€ì¦í•  ë•Œ ì‚¬ìš©

![[F3F29782-7418-4F52-B9A4-0CE4AFF18248.jpeg]]

~~~python
# %%
from sklearn.model_selection import StratifiedKFold
data=np.array(range(10))
label=np.array([0,0,0,0,0,1,1,1,1,1])
exam_folds=StratifiedKFold(n_splits=5,shuffle=False) #ì…”í”Œì˜ ì—¬ë¶€ë¥¼ ê²°ì •
for train_index,test_index in exam_folds.split(data,label):
    print('train_index:',data[train_index],'test_index:',data[test_index])
~~~

### 7.2. ì‹¤ìŠµ

~~~python
# %%
from sklearn.model_selection import KFold
folds=KFold(n_splits=5,shuffle=True)#5ê°œì˜ foldë¡œ ë‚˜ëˆ„ê³  ì…”í”Œì„

#êµì°¨ê²€ì¦ì€ np.arrayí˜•íƒœë¡œë§Œ ê°€ëŠ¥í•´ np.arrayë¡œ ë³€í™˜
X=np.array(X_train)
y=np.array(log_y_train)

RMSLE_history=[]#ê²€ì¦ê²°ê³¼ë¥¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸
result_history=pd.DataFrame()#ê²€ì¦ê²°ê³¼ë¥¼ ì €ì¥í•  ë°ì´í„°í”„ë ˆì„

num=0
for train_index,test_index in folds.split(X):
    X_train_fold,X_test_fold=X[train_index],X[test_index]
    y_train_fold,y_test_fold=y[train_index],y[test_index]

    RF_model2= RandomForestRegressor(n_estimators=175,max_depth=18)

    y_pred=RF_model_2.predict(X_test_fold)
    RMSLE_history.append(RMSLE(y_test_fold,y_pred))

  result_history['test'+str(num)]=RF_model_2.predict(X_test)
  num+=1

#5ê°œì˜ êµì°¨ ê²€ì¦ test ì˜ˆì¸¡ ê²°ê³¼ì˜ í‰ê· ê³¼ RMSLEë„ì¶œ
# %%
print(np.array(result_history).mean(axis=1).reshape(len(X_test),1))

print(RMSLE(log_y_test,np.array(result_history).mean(axis=1).reshape(len(X_test),1)))
~~~
