---
title: 빅데이터 분석 오픈북 참고용 노트 필기 정리
categories:
  - KNU
tags:
  - Statistics
  - BigData
toc: true
---

## 1. 로지스틱 회귀분석

지금까지 회귀분석은 직선형 이나 2차 곡선의 형태로 나타낼수있었다. 로그를 취해서 다중회귀로 만드는것은 지수함수를 로그를 취하는 방식을 사용한다. 로그 리그레이션==로지스틱리그레이션

s형태의 곡선을 나타내게된다. 지금까지 종속변수는 수치로 나타내고있었다. 독립변수x에대한 종속변수y는 문자형으로 나타낼수있게된다. 성공이라는 문자를 나타낼수있는 오즈비를 나타낸다. 성공/실패을 `승산`비를 의미한다.  

여기에서 독립변수x는 `설명변수` 척도y는 bool 2점 척도, 3개면 3점척도 5개면 5점척도....

로지스틱의 정의 종속변수가 범주형 문자형 변수일때 독립변수와 종속변수와의 관계를 찾아야한다. 두개의 범주(카테고리)가 문자형 데이터일때 로지스틱 회귀분석이다. 범주형:categorical data

종속변수는 목표변수이다.x는 설명 독립변수 또는 예측변수 y는 종속변수이다. 로지스틱은 독립변수에대한 과정이 필요없다. 독립변수에 대한 조건이 필요하면 판별분석 필요없으면 로지스틱.  오즈(odds)비: 확률/발생안함
0.5보다 크면 발생, 0.5보다 작으면 안발생

로지스틱 강의의 6페이지 참조: prob(구매)=1/1+e^(-z)

z는 판별분석처럼 B * 대상의 value를 사용한다(대도시거주 1 아님 0 이고 B가 0.123일때 대도시 거주라면 1 * 0.123) e는 지수함수

z는 1.246일때 e^-10246=0.289653이므로 1/1.289653=0.7766이므로 0.5보다 높다 따라서 구매 확률은 높다 볼수있다.

12페이지 B가 음수면 구매 하지 않을 방향의 점수이다. 얼굴부우희망은 -.769로 높다? 높을수록 p-value는 낮아진다. 왜 영향도가 큰것이 입증되고있기에

14페이지 실제데이터를 투입하여 그 결과를 나타내는 테이블이다. 왜 62퍼따리인가? p-value 0이 별로 없다! 

`15페이지` `중요``중요``중요``중요``중요``중요``중요` 

**13-10 그림을 참고로 결과도출**

1.  <br>H0 관측 빈도와 예측 빈도는 차이가 없다<br>H1 관측 빈도와 예측 빈도는 차이가 있다

2. <br>a=0.05

3. <br>검정통계량<br> 
kai-sq=6.905<br>
p=0.547

4. <br>기각역<br>
주어진 데이터 98개 에서는 관측값과 예측값이 차이가 없다. 그러므로 fitting모델은 정확하다.

16페이지 못함 보통 잘함의 3개 기준? 3중 1을 분모로둔다 잘함/보통 못함/보통의 형태를 사용한다. 18페이지 참고

## 2.판별분석

`피셔분류함수`이나 `판별규칙(함수)`을계산하는것이 `판별분석`이다.

데이터 각 개별 케이스가 어느 집단을 분류되는지를 사전에 알고있어야한다. 
종속변수는 집단을 구준하는 명목변수이어야한다.

B 계수추정은 weight를 추정하는것이다,

SDA12_판별분석의 12-6에 나오는 정준판별함수계수를 통해 계산한다. 판별함수의 갯수는 구별하고자하는 집단의 수이다. 하나 작은 개수와 독립변수의 개수이므로 구매 비구매 2개이므로 2-1=1개이다 따라서 구매에 관련된 함수는 구매에 관련된 하나의 데이터를 가진다.

|구분|함수1|
|-|-|
|나이|.034|
|대도시거주|-.596|
|기능성식품경험|.759|
|키|-.051|
|결혼여부|1.041|
|성별|1.1517|
|...|...|

input:

`29`세 `대도시`에 사는 `기능성식품경험없는` 키 `151cm`의 `미혼` `여성` 

output:

**판별점수**

29 * 0.034+
1 * 1.041+
0 * -.596+
151 * -0.051+
0 * 1.041+
1 * 1.1517+
... 

**결과판독**

판별점수의 히스토그램 도표의 집단 중심값을 계산하여 이를 판별의 기준으로 삼는다.


판별함수를 사용해서도 예측가능하다 피셔 알고리즘을 사용하는경우:

구매 안함/ 구매함이면 분류함수는 2개 로 n개 만킁 나온다. 이때 총합이 가장 큰 분류 함수가 예측되는 값이다.

F분포를 통해 판별점수 편균이 집단과 유의한 차이가있는지 검정 가능하다. 집단간 유의미 차이를 통해 
이 판별함수를 예측함수로 사용가능한지, 집단별 유의미한 차이가있는지, 구매와 비구매가 유의하게 구분이 되어지는지 확인가능하다.




## 2.1 로지스틱-판별분석의 차이점

판별분석 노멀디스트리뷰션- 같은결과를 얻는y가 필요하다
로지스틕은 데이터의 조건이 없다

둘다 분류를 답으로 내놓는다.
로지스틱-분류를 위한 무언가가 좋고 나쁘고...
판별분석도 범주형 자료로 분류해주는것

수치로하는건 예측뿐

## 3. 의사결정나무

나무구조는 문자로 예측하는 방법에 속한다. 분류 기준이 되는것은 독립변수이며 y에 영향력이있는 x이다.
의사결정규칙을 데이터를 통해 이루어진다. 이런 모델을 만드는 이유는 이미 만들어진 조건을 사용해 
미래의 타킷에게 적용하기위해서이다.

나무모형은 분류, 등급화, 세분화, 변수선택, 상호작용 탐색 용도로 사용가능하다.

최상단 뿌리노드 -> 기여도가 높은 자식노드 -> y값을 구할수있는 최종노드 

하나의 나무구조만으로는 학습데이터의 작은 변화로 인해 출력이 크게 변할수있다는 불안정성이있다. 따라서 이를 해결할 방법이 필요한데 이를 앙상블방법이라 한다.

## 3.1. 앙상블방식

배깅, 랜덤포레스트, 부트스트랩(교재에 안나옴)

### 3.2. 배깅

여러개의 `bootstrap`을 사용해서 각 bootstrap 데이터 예측모형을 결합(평균)해 최종 예측모형을 만든다.


<div class="notice">
<h4>
부트스트랩:
주어진 데이터로부터 동일 크기 표본을 랜덤 복원 추출로 뽑은 데이터(5개 종이가있다면 하나 꺼내고 적고 다시넣고....5개를 랜덤 뽑은 데이터를 5개)를 이용해서 예측모형을 구성한다.
</h4>
</div>


배깅시 의사 결정나무 구축시 예측력이 크게 향상되고 가지치기의 수가 줄어드는 이점을 가진다.

**트레이닝 데이터**
모형 데이터와 테스트를 위한 트레이닝(학습) 데이터를 분리하고 테스트 하는 데이터를 분리해 검증해야한다. 

### 3.3. 랜덤포레스트

배깅보다 더 많은 무작위성을 주어 예측모형들을 생성한뒤 선형결합(덧셈)하여 만든 최종 학습기. 배깅의 예측 모형(나무들)을 랜덤 선별한다.

강의록 12 14페이지 참고

- 매우 높은 에측력
- 해석의 어려움을 가짐


## 4. 벌점화 회귀분석

상관분석 2개의 변수 데이터 필드의 관게성을 해석하고 알아보기위함에 있다.

수업과 저의 상관계수는 -1에 가까운것같습니다..그래도 포기는안했습니다..
대충 -.5에 가까운것같습니다 죄송합니다...

로지스틱과 판별분석은 종속변수가 문자형이라는 유사성을 가지고있다. output이 문자형이라는 것

벌점화회귀분석:고차원 자료에서의 회귀분석이다. 

**review**
회귀분석: 종속변수 y와 p개의 독립변수x1...xp들 사이의 함수 관계를 다음과 같이 모형화하는것
Y=f(x1.....xp)+Ɛ(앱실론으로서...오차(편차,잔차))

선형회귀모형은 회귀함수 f가 선형함수이다.
f(x1.....xp)= β0+ β1x1+ βpxp

회귀계수(베타) 추정: 최소제곱 추정량으로 추정
 β^=argminβΣ(yi-x'iβ)^2

데이터의 형테(데이터셋)(xi,yi)

고차원 선형회귀모형: <br>선형 회귀모형에서 설명변수 수p가 데이터 수 n보다 큰 경우 (p>n)
(ex: 웹사이트 테이터는 하나n인데 선택가능한건 무지많음) 

(최소제곱 추정량으로 구하는공식) 𝛽^(계수) = (𝑋′𝑋)^(−1)𝑋′𝑦, (𝑋′𝑋)^(−1)는 역행렬 이게 있어야 계산 가능 근데 p>n이면 그게 정의될수가 없다.

이럴때는 최소제곱추정량이 불가능하다! 따라서 변수 선택의 방식에 변화를 주어야한다. 
안그러면 추정결과의 불안정과 회귀모형의 예측력이 저하된다.

이를위하여 벌점(패널티)으로 가중치를 사용하여 벌점화 회귀모형이 생겨남

`𝐽𝜆`벌점함수 
`𝜆`는 양의 실수(𝜆<=0)로 `조율모수`이며 벌점함수가 추정량에 미치는 영향을 조절한다. 모든 베타를 0으로 두는 추정값을 얻을수있다. 계수가 0이된다? 독립변수가 없어진다 이를 통해 변수선택이 가능해진다. 람다가 크면 베타가 0이되는 현상이 나타난다. 벌점을 많이먹으면 없어진다. x가 없어진다? 필요없는 변수를 제거가능하다.
`𝑦𝑖 − 𝑥𝑖′𝛽`는 잔차이다.
`|𝛽k|`k번째에서의 패널티 값(회귀계수)

>argmin𝛽∑(𝑦𝑖 − 𝑥𝑖′𝛽)^2 + ∑ 𝐽𝜆 (|𝛽k|)


고차원모형에서 벌점화방법으로 잔차(편의,bias)가 생기지만 `분산`을 크게 줄일수있다. 

- 분산: 평균을 중심으로 퍼짐의 정도


### 4.1. 능형회귀(ridge regression)

review에서 말한것과같이 p>n일때는 최소제곱추정량을 못 구한다 그래서

능형 추정량이란 벌점함수 𝐽𝜆 (|𝛽|)를 𝜆𝛽^2로 사용하여 구한 벌점화 최소추정량이다.

>argmin𝛽∑(𝑦𝑖 − 𝑥𝑖′𝛽)^2 + 𝜆∑𝛽k^2

역행렬이 존재할수있도록 𝑋′𝑋을 ->𝑋′𝑋+`𝜆I`(I는 identity matrix로 단위행렬 스칼라1의 의미를 가지는 메트릭스)로 수정하여 만들어진 최소제곱추정량을 능형 추정량이라고 한다

>𝛽^(계수) = (𝑋′𝑋+𝜆I)^(−1)𝑋′𝑦

- 결론: p>n일때 역행렬을 만들어주는것 그것이 능형회귀이다!

### 4.2. 라쏘회귀

능형 추정량은 모든변수를 사용한다. 따라서 회귀모형의 해석이 어렵다!

람다베타(𝜆|𝛽|)를 이용한 라쏘 벌점함수를 이용한 벌점화 최소제곱추정량을 구할수있다.

>argmin𝛽∑(𝑦𝑖 − 𝑥𝑖′𝛽)^2 + 𝜆∑(|𝛽k|)

**성긴성질**

추정된 회귀계수중 정확히 0이되는 회귀계수가 존재한다. 

- 결론: 라쏘회귀는 변수를 줄인다!


### 4.3. 로지스틱 회귀모형의 개요

로그가능도함수l(𝛽)를 사용한다. 베타 l(𝛽)를 최대로 하는 최대 가능도 추정량을 사용한다. 그 알고리즘은 IRWLS으로 불린다.

공변량의 차원이 크기때문에 주어진 벌점함수

l(𝛽)-∑𝐽𝜆(|𝛽j|)를 최대화하는 벌점화 최대 가능도 추정량을 사용한다.

## 5. 균집분석


분류-균집분석하나만 배운다.


**review**
회귀를 공부했는데 상관분석 코릴레이션 애널리시스- 상관분석: x(독립변수)만 가지고 분석, x중에서 분석의 대상이되는것인 하나를 y로 바꾸는것 이 y는 숫자로만 나타난다. 로지스틱은 y를 문자를 사용한다 이것을 분류라고 한다(예측이 문자면 분류)
로지스틱은 나머지x조건이 붙지 않는다. 
판별분석-정리정돈된 데이터가 노멀디스트리뷰션(정규화된 데이터)인경우에는 로지스틱보다 판별이 더 좋다. 분류의 정확도가 더 높기때문, 판별함수와 분류함수를 사용하는 두가지 방식이있다. 그 중간 역할의 하는것은 계층적 의사결정나무를 통해(예측모형) 문자형 예측, 수치형 예측 가능, 그런데 샘플사이즈에 비해 x가 너무 많다!? 앙상블 방식을 통해 여러 의사결정 나무를 병합(어그리게이션)하여 예측한다. 부트스트랩을 사용하는 배깅이있고 이는 랜덤 샘플링데이터를 복원추출방식을 사용하여 만들어내 만들어내는 방식이다.  
추가:랜덤하지 않으면 오차가 커지게된다. 오차나 잔차를 구할때는 표준편차를 구하기에 푱균을 사용한다. 분산을 루트씌우면 표준편차가된다. 

나무의 그루수를 랜덤하게 뽑아서 랜덤 병합하는것은 랜덤 포레스트 방식이다.

군집분석: 데이터를 분류하여 클러스터별(범주)로 모은다. 그 클러스터의 특징을 알아내는것이다.
모집단, 범주에대한 사전정보가 없는경우에 사용하는 비지도학습이다.

+ 비지도학습 = unsupervised learning , 예측모형은 모두 관리학습이다 왜 x가 정해져있기때문. 분류모형은 비지도학습이다. +

각 군집 별 특정 파악 : 각 군집내의 개체들을 다른 군집에 속하는 개체들보다 서로 `더 유사하도록` 여러 군집으로 나눈 후 각 군집 별 특성 파악 특이 값을 갖는 개체 발견 ,결측 값의 보정

최단연결법:
 각 군집에서 하나씩 관측 값을 뽑았을 때 나타날 수 있는 모든 조합의 거리의 최소값
 최장연결법:
 각 군집에서 하나씩 관측 값을 뽑았을 때 나타날 수 있는 모든 조합의 거리의 최대값

k-평균 군집분석- 장점:데이터 계산량이 적다. 단점:문자형데이터 사용불가

일반적으로 𝑘값이 증가(군집수가 많을수록)함에 따라 거리 제곱합 감소
거리 제곱합: 클러스터 내 (군집내) 데이터 간의 거리 값 

k값으로 인한 거리제곱합 변동이 없을때까지 늘려서 그중 가장 작은 k를 구할수있다

알고리즘:

1. 군집수 𝑘가 주어지면 랜덤하게 초기 𝑘개 군집의 중심(`평균`) 선택
2. 각 관측 값을 그 중심과 가장 가까운 거리에 있는 군집에 할당
3. 군집 중심을 새로 계산
4. 기존의 중심과 새로 계산된 중심 간에 차이가 없을 때까지 2-3번 반복 클러스터링 끝.


병렬처리 알고리즘:

1. 데이터의 수를 𝑛개 slave node의 수를 𝑝개라 하자. 각 slave cpu에 𝑛/𝑝개의 데이터를 할당한다.

2. Mater cpu는 각 slave에 현재의 𝑘개의 군집의  평균을 보낸다.

3. Slave는 주어진 𝑘개의 평균을 이용하여 데이터를 군집으로  나눈 후, 새로운 군집의 평균을 Master로 보낸다.

4. Master에서 각 slave에서 구한 군집들의 평균을 구한다.

5. 2-4단계를 Master에서 구한 군집들의  평균에 수렴할 때까지 반복
