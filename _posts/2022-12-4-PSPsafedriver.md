---
title: ë¬¸ì œí•´ê²°í”„ë¡œê·¸ë˜ë° ì•ˆì „ ìš´ì „ì ì˜ˆì¸¡[KNU 2022-2][kaggle]
categories:
  - KNU
tags:
  - ML
toc: true
---

# ğŸ‘¨â€ğŸ’»ğŸ«KNU 2022-2 SW & media ë¬¸ì œí•´ê²°í”„ë¡œê·¸ë˜ë° ì•ˆì „ ìš´ì „ì ì˜ˆì¸¡

## 0.  Working Pipeline

| ìˆœì„œ | ì´ë²ˆ ê³¼ì œì—ì„œëŠ”.. |
|-|-|
|ê²½ì§„ëŒ€íšŒì˜ ì´í•´| 1. ì•ˆì „ ìš´ì „ìì—ê²Œ ì ì€ ë³´í—˜ë£Œ, ë‚œí­ ìš´ì „ìì—ê²Œ ë§ì€ ë³´í—˜ë£Œë¥¼ ë¶€ê³¼í•˜ê¸° ìœ„í•´ <br>&nbsp;&nbsp;&nbsp;&nbsp;ìš´ì „ìê°€ ë³´í—˜ê¸ˆì„ ì²­êµ¬í•  í™•ë¥ ì„ ì˜ˆì¸¡í•˜ëŠ” ëª¨ë¸ ê°œë°œ<br>2. ìµœì¢…ì ìœ¼ë¡œ í™•ë¥ ì„ ê¸°ë°˜ìœ¼ë¡œ ìš´ì „ìê°€ ë³´í—˜ê¸ˆì„ ì²­êµ¬í• ì§€(1) ì•„ë‹ì§€(0) ì˜ˆì¸¡ |
|íƒìƒ‰ì  ë°ì´í„° ì´í•´|1. ë°ì´í„° íŒŒì•…(feature aggregation)<br>2. ë°ì´í„° ì‹œê°í™”(ê°ì¢… í”¼ì³ì˜ ë¶„í¬í™•ì¸)<br>&nbsp;&nbsp; 2.1. íƒ€ê²Ÿê°’<br>&nbsp;&nbsp; 2.2. ì´ì§„í”¼ì³<br>&nbsp;&nbsp; 2.3. ìˆœì„œí˜•í”¼ì³<br>&nbsp;&nbsp; 2.4. **ì—°ì†í˜•í”¼ì³**<br> 3. ë¶„ì„ì •ë¦¬ ë° ëª¨ë¸ë§ ì „ëµ ìˆ˜ë¦½|
|ë² ì´ìŠ¤ë¼ì¸ ëª¨ë¸|LightGBMì‚¬ìš©|
|ì„±ëŠ¥ ê°œì„ |LightGBM + Improved Feature Engineering, Hyper-parameter Optimization(HPO)|

## 1. ë°ì´í„° íŒŒì•…(feature aggregation)

~~~python
# %%
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt

# %%
train = pd.read_csv('train.csv')
test = pd.read_csv('test.csv')

pd.set_option('display.max_columns', 100)
train.head()

# %%
print(train.shape,test.shape)
# %%
train.drop_duplicates()
test.drop_duplicates()
print(train.shape,test.shape)
# %%
train.info() 

# %%
train_copy = train.copy().replace(-1, np.nan) #  ì¹´í”¼ ë°ì´í„°ì— ê²°ì¸¡ì¹˜ì¸ -1ì„ nanìœ¼ë¡œ ë³€ê²½
import missingno as msno
msno.bar(train_copy, figsize=(20, 8))

# %%
def resumetable(df):
    print(f"Dataset Shape: {df.shape}")
    summary = pd.DataFrame(df.dtypes, columns=['dtypes'])
    summary['Missing'] = (df==-1).sum().values #ê²°ì¸¡ê°’
    summary['Uniques'] = df.nunique().values #ê³ ìœ ê°’
    for col in df.columns:
        try:
            if 'bin' in col or col=='target': #ì´ì§„í˜•
                summary.loc[col, 'data type'] = 'binary'
            elif 'cat' in col: #ëª…ëª©í˜•
                summary.loc[col, 'data type'] = 'categorical'
            elif df[col].dtype == float: #ì—°ì†í˜•
                summary.loc[col, 'data type'] = 'float'
            elif df[col].dtype == int: #ìˆœì„œí˜•
                summary.loc[col, 'data type'] = 'integer'
        except:
            pass
    return summary

# %%
summary=resumetable(train)
summary
~~~

## 2. ë°ì´í„° ì‹œê°í™”

### 2.1. ì´ì§„í˜• targetë³€ìˆ˜ì˜ 0ê³¼ 1ì˜ ë¹„ìœ¨ì„ í™•ì¸

~~~python
# %%
import matplotlib as mpl
mpl.rc('font', family='Malgun Gothic', size=12)

plt.figure(figsize=(7,6))
ax=sms.countplot(x='target',data=train)

# ì´ì§„í˜• í”¼ì³ë“¤ì˜ ë¶„í¬(í¼ì„¼íƒ€ì¼ ê°’)ë¥¼ í™•ì¸í•´ë³¸ë‹¤.
def write_percent(ax, total_size):
    for p in ax.patches:
        ax.text(p.get_x() + p.get_width() / 2.,\
             p.get_height()+total_size*0.01,\
            '{:0.1f}%'.format(100 * p.get_height()/total_size),\
            fontsize=12, color='black', ha='center', va='bottom')

write_percent(ax, len(train))
ax.set_title('Target Distribution')
~~~

### 2.2. í”¼ì³ì—ì„œì˜ targetê°’ì˜ ë¹„ë¥¼ í™•ì¸

~~~python
# %%
import matplotlib.gridspec as gridspec

# %%
def plot_target_by_feature(df, feature, num_rows, num_cols, figsize=(20, 6)):
    mpl.rc('font', family='Malgun Gothic', size=12)
    plt.figure(figsize=figsize)
    gs = gridspec.GridSpec(num_rows, num_cols)
    plt.subplots_adjust(wspace=0.3, hspace=0.3)

    for idx, col in enumerate(df[feature]):
        ax = plt.subplot(gs[idx])
        sns.barplot(x=feature, y='target', data=df, ax=ax, palette='Set2')

# %%
#ì´ì§„í˜• íŠ¹ì„± ì •ì˜        
binary_features = summary[summary['data type']=='binary'].index
plot_target_by_feature(train, binary_features, 7, 2, figsize=(20, 12))

# %%
#ì—°ì†í˜• íŠ¹ì„± ì •ì˜
cont_features = summary[summary['data type']=='float'].index

plt.figure(figsize=(12, 16))
grid=gridspec.GridSpec(5,2)
plt.subplots_adjust(wspace=0.2, hspace=0.4)

for idx, col in enumerate(df[cont_features]):
    train[col]=pd.cut(train[col], 5)

    ax=plt.subplot(grid[idx])
    sns.barplot(x=cont_features, y='target', data=train, ax=ax, palette='Set2')
    ax.tick_params(axis='x', rotation=90)
~~~

1. ì‹¤í–‰ê²°ê³¼ ëª…ëª©í˜• í”¼ì³ì˜ ê²°ì¸¡ê°’ì´ ë§ì€ ê²ƒì„ í™•ì¸í•  ìˆ˜ ìˆë‹¤. ê·¸ëŸ¬ë‚˜ ì´ëŸ¬í•œ ê²°ì¸¡ê°’ì— íƒ€ê¹ƒì— ëŒ€í•œ ì˜ˆì¸¡ë ¥ì´ ìˆë‹¤ë©´ ì´ë¥¼ ê³ ìœ ê°’ìœ¼ë¡œ ì·¨ê¸‰í•˜ì—¬ í”¼ì³ë¡œ ì‚¬ìš©í•  ìˆ˜ ìˆë‹¤.

2. ì‹ ë¢°êµ¬ê°„ì´ ë„“ì–´ë„ í”¼ì³ë¥¼ ì œê±°í•˜ì§€ ì•ŠëŠ” ê²½ìš°ë„ ìˆìœ¼ë©° ë‹¤ë¥¸ íŠ¹ì„±ì˜ ì‹ ë¢°êµ¬ê°„ ë‚´ì— ê²¹ì¹¨ì´ ì—†ì´ ì°¨ì´ê°€ í° ê²½ìš°ê°€ í•´ë‹¹ëœë‹¤.

## 3. í”¼ì³ ì—”ì§€ë‹ˆì–´ë§

### 3.0. ë°ì´í„° ì¤€ë¹„

~~~ python
#ë°ì´í„° ì¤€ë¹„ì˜ í¸ì˜ì„±ì„ ìœ„í•´ trainê³¼ testë¥¼ í•©ì¹œë‹¤.
# %%
all_data = pd.concat([train, test], axis=0, ignore_index=True)
all_data.drop('target', axis=1, inplace=True)
all_data.columns
~~~

### 3.1. ë°ì´í„° ì¸ì½”ë”©

- ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìì˜ í˜•íƒœë¡œ ë°”ê¾¸ëŠ” ì‘ì—…
- ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ì€ ë¬¸ìë°ì´í„°ë¥¼ ì¸ì‹í•˜ì§€ ëª»í•˜ê¸°ì— ë¬¸ìí˜• ë²”ì£¼ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ë³€ê²½í•´ì•¼í•  í•„ìš”ê°€ ìˆë‹¤.
- ë ˆì´ë¸” ì¸ì½”ë”©, ì›-í•« ì¸ì½”ë”© ë“±ì„ ì‚¬ìš©

#### 3.1.1. ë ˆì´ë¸” ì¸ì½”ë”©

- ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ì¼ëŒ€ì¼ ë§¤í•‘í•´ì£¼ëŠ” ì¸ì½”ë”©, ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ìë¡œ ì¹˜í™˜
- ëª…ëª©í˜•ë°ì´í„°ë¥¼ ë ˆì´ë¸” ì¸ì½”ë”©ì‹œ ëª¨ë¸ ì„±ëŠ¥ ì €í•˜ì˜ ê°€ëŠ¥ì„±ìˆìŒ(êµ¬ë¶„ìš©ìœ¼ë¡œ 1,2,3ê³¼ ê°™ì´ ë³€í™˜ ë ê²½ìš° 1,2 ë˜ëŠ” 2,3ì„ 1,3ë³´ë‹¤ ë¹„ìŠ·í•œ ë°ì´í„°ë¡œ ì—¬ê¹€)
- ex)

~~~python
# %%
from sklearn.preprocessing import LabelEncoder
cat = ['A', 'B', 'C', 'C']
le = LabelEncoder()
cat_lab=le.fit_transform(cat)
print(cat_lab)

#ì´í•˜ ì¶œë ¥
# 0 1 2 2
~~~

#### 3.1.2. ì›-í•« ì¸ì½”ë”©

- ì—¬ëŸ¬ ê°’ ì¤‘ í•˜ë‚˜ë§Œ í™œì„±í™”í•˜ëŠ” ì¸ì½”ë”©
- ì¸ì½”ë”© í•˜ê³ ìí•˜ëŠ” íŠ¹ì„±ì˜ ê³ ìœ ê°’ ê°œìˆ˜ë¥¼ êµ¬í•œë‹¤
- íŠ¹ì„±ì˜ ê³ ìœ ê°’ ê°¯ìˆ˜ë§Œí¼ ì—´ì„ ì¶”ê°€í•œë‹¤
- ê° ê³ ìœ ê°’ì— í•´ë‹¹í•˜ëŠ” ì—´ì— 1ì„ í‘œì‹œí•˜ê³  ë‚˜ë¨¸ì§€ ì—´ì— 0ì„ í‘œì‹œ
- ex)

~~~python
# %%
from sklearn.preprocessing import OneHotEncoder

ohe = OneHotEncoder()
cat_ohe = ohe.fit_transform(cat_lab.reshape(-1,1))
print(cat_ohe.toarray())

#ì´í•˜ ì¶œë ¥
# [[1. 0. 0.]
#  [0. 1. 0.]
#  [0. 0. 1.]
#  [0. 0. 1.]]
~~~

- ëª…ëª©í˜• ë¬¸ìì—´ ë°ì´í„°ë¥¼ ì›-í•« ì¸ì½”ë”©ìœ¼ë¡œ ë³€í™˜ í•˜ê¸°ìœ„í•´ì„œëŠ” pd.get_dummies()í•¨ìˆ˜ë¥¼ ì‚¬ìš© ê°€ëŠ¥
  - ex)

~~~python
# %%
pd.get_dummies(cat)

#ì´í•˜ ì¶œë ¥
#    A  B  C
# 0  1  0  0
# 1  0  1  0
# 2  0  0  1
# 3  0  0  1
~~~

#### 3.1.3. ë°ì´í„°ì— ì¸ì½”ë”© ì ìš©

ë°ì´í„° ì¤‘ 'cat'ì´ ë“¤ì–´ê°€ëŠ” íŠ¹ì„±ì€ ëª¨ë‘ ëª…ëª©í˜• ë°ì´í„° ì´ë¯€ë¡œ ì›-í•« ì¸ì½”ë”©ì„ ì ìš©í•œë‹¤.

~~~python
# %%
cat_features = [col for col in all_data.columns if 'cat' in col]
cat_features

# %%
encoded_cat = OneHotEncoder().fit_transform(all_data[cat_features]).toarray()

#[2.2]ì—ì„œ í™•ì¸í•œ ë¶ˆí•„ìš”í•œ íŠ¹ì„±ì„ ì œê±°í•˜ê³  ë‚¨ì•„ìˆëŠ” íŠ¹ì„±ì„ ì¬êµ¬ì„±í•œë‹¤
# %%
drop_features = [ë¶ˆí•„ìš”í•œ íŠ¹ì„±]
remaining_features = [\
    col for col in all_data.columns \
    if(\
        'calc' not in col and \
        'cat' not in col and \
        col not in drop_features \
        )\
    ]

#ì´í›„ ë‚¨ì•„ìˆëŠ” íŠ¹ì„±ê³¼ ì¸ì½”ë”©í•œ ê²°ê³¼ë¥¼ í•©ì³ CSRí–‰ë ¬ë¡œ ì¬êµ¬ì„±í•œë‹¤
# COO, CSR í–‰ë ¬ì€ í¬ì†Œí–‰ë ¬ì„ í‘œí˜„í•˜ëŠ” ë°©ë²•, ì°¸ê³ (https://wooono.tistory.com/29)
# %%
from scipy import sparse
all_data_sparse = sparse.hstack([\
    sparse.csr_matrix(all_data[remaining_features]),\
    encoded_cat\
    ],\
    format='csr')
~~~

### 3.2. í”¼ì³ìŠ¤ì¼€ì¼ë§

- ì„œë¡œ ë‹¤ë¥¸ íŠ¹ì„± ê°’ì˜ ë²”ìœ„(MIN-MAX)ê°€ì¼ì¹˜í•˜ë„ë¡ ì¡°ì •í•˜ëŠ” ì‘ì—…
- ê°’ì˜ ë²”ìœ„ê°€ ë°ì´í„°ë§ˆë‹¤ ë‹¤ë¥´ë©´, ëª¨ë¸ í›ˆë ¨ì´ ì œëŒ€ë¡œ ì•ˆë  ê°€ëŠ¥ì„±ìˆìŒ
- ì¢…ë¥˜ë¡œëŠ” Min-Max normalixation, standardizationì´ ìˆìŒ
- íŠ¸ë¦¬ê¸°ë°˜ ëª¨ë¸ì—ì„œëŠ” ì˜í–¥ì„ ë°›ì§€ ì•ŠìŒ->ë°ì´í„° í¬ê¸°ë³´ë” ëŒ€ì†Œê´€ê³„ ì˜í–¥ì´ í¬ê¸° ë•Œë¬¸

#### 3.2.1. Min-Max normalixation

$$x_{scaled}=\frac{x-x_{min}}{x_{max}-x_{min}}$$

- ex)

~~~python
# %%
df=pd.DataFrame({\
    'x':[1.7,1.5,1.8]\
    'y':[75,55,60]\
    })

# %%
from sklearn.preprocessing import MinMaxScaler

# %%
scaler_M = MinMaxScaler()
df_scaled_M = scaler_M.fit_transform(df)
df_scaled_M

#ì´í•˜ ì¶œë ¥
# array([[0.66666667, 1.        ],
#        [0.        , 0.        ],
#        [1.        , 0.5       ]])
~~~

#### 3.2.2. Standardization

ì •ê·œë¶„í¬ë¥¼ ë”°ë¥´ëŠ” ë°ì´í„°ëŠ” í‘œì¤€í™” ìŠ¤ì¼€ì¼ë§ì„ ì‚¬ìš©í•˜ëŠ”ê²Œ ì¢‹ë‹¤

- $\bar{x}$: íŠ¹ì„±ì˜ í‰ê· 
- $\sigma$: íŠ¹ì„±ì˜ í‘œì¤€í¸ì°¨

$$x_{scaled}=\frac{x-\bar{x}}{\sigma}$$

- ex)

~~~python
# %%
from sklearn.preprocessing import StandardScaler

# %%
scaler_S = StandardScaler()
df_scaled_S = scaler_S.fit_transform(df)
df_scaled_S

#ì´í•˜ ì¶œë ¥
# array([[ 0.26726124,  1.37281295],
#        [-1.33630621, -0.98058068],
#        [ 1.06904497, -0.39223227]])
~~~

### 3.3. ë°ì´í„° ì™„ì„±

~~~ python
num_train = len(train)
X_train = all_data_sparse[:num_train]
X_test = all_data_sparse[num_train:]
y_train = train['target'].values
y_test = test['target'].values
~~~

## 4. Boosting

> ìƒìœ„ ê°œë…ì€ ë‹¤ìŒì„ ì°¸ê³ [[ìì „ê±° ëŒ€ì—¬ ìˆ˜ìš” ì—ì¸¡#ì•™ìƒë¸” ê¸°ë²•]]

ê°€ì¤‘ì¹˜ë¥¼ í™œìš©í•˜ì—¬ ë¶„ë¥˜ì„±ëŠ¥ì´ ì•½í•œ ëª¨ë¸ì„ ê°•í•˜ê²Œ ë§Œë“œëŠ” ê¸°ë²•!

[[446808CF-9BA4-462E-B30E-8AF2E26EF2E7.jpeg]]

ëŒ€í‘œì ì¸ ì¢…ë¥˜ë¡œëŠ” XGBoostì™€lightGBMë“±ì´ ìˆë‹¤.

- XGBoost - ê· í˜• ì¤‘ì‹¬ ë¶„í• ë¡œ ë³‘ë ¬ ë°°ì¹˜
- LightGBM - ë§ë‹¨ë…¸ë“œ ì¤‘ì‹¬ ë¶„í• ë¡œ ì§ë ¬ë°°ì¹˜(ì†ë„ê°€ ë¹ ë¥´ì§€ë§Œ ë°ì´í„° ìˆ˜ê°€ ì ì„ë•ŒëŠ” ê³¼ì í•©ë˜ê¸° ì‰½ë‹¤)

~~~python
# ê·¸ë¦¬ë“œ ì„œì¹˜ê°€ ì ìš©ëœ LightGBM ì‹¤ìŠµ
# %%
from lightgbm import LGBMClassifier
from sklearn.model_selection import GridSearchCV

# %%
param_grid={
    'max_depth':[range(10,30,10)],\
    'random_state':[42],\
}

# í•™ìŠµëª¨ë¸, ì„œì¹˜í•  íŒŒë¼ë¯¸í„°, cvíšŸìˆ˜, ì‚¬ìš©í•  cpuê°œìˆ˜ (n_jobs=-1: ëª¨ë“  cpuì‚¬ìš©)
grid = GridSearchCV(LGBMClassifier(), param_grid, cv=5, n_jobs=-1)
grid.fit(X_train, y_train)

# ê°€ì¥ ì¢‹ì€ íŒŒë¼ë¯¸í„° ì¶œë ¥
# %%
grid.best_params_

# ëª¨ë¸ ì¬ì •ì˜ ë° ì˜ˆì¸¡
# %%
LGBMmodel=LGBMClassifier(max_depth=grid.best_params_['max_depth'], random_state=grid.best_params_['random_state'])
LGBMmodel.fit(X_train, y_train)

pred_train = LGBMmodel.predict(X_train)
pred_test = LGBMmodel.predict(X_test)
~~~

## 5. ë¶„ë¥˜ë¬¸ì œì˜ í‰ê°€ì§€í‘œ

1. ì˜¤ì°¨í–‰ë ¬
2. ë¡œê·¸ì†ì‹¤
3. ROC-Curveì™€ AUC

### 5.1. ì˜¤ì°¨í–‰ë ¬

| | | |
|-|-|-|
|*ì‹¤ì œìŒì„±*|**ì°¸ìŒì„±**|**ê±°ì§“ì–‘ì„±**|
|*ì‹¤ì œì–‘ì„±*|**ê±°ì§“ìŒì„±**|**ì°¸ì–‘ì„±**|
|            |*ì˜ˆì¸¡ìŒì„±*|*ì˜ˆì¸¡ì–‘ì„±*|

ì´ë¥¼ í†µí•˜ì—¬  4ê°€ì§€ í‰ê°€ì§€í‘œë¥¼ ì–»ì„ ìˆ˜ìˆë‹¤.
  
- ì •í™•ë„: ì‹¤ì œê°’ê³¼ ì˜ˆì¸¡ê°’ì´ ì¼ì¹˜ë˜ëŠ” ë¹„ìœ¨. ë†’ì€ ì •í™•ë„ê°€ ìš°ìˆ˜í•œ ëª¨ë¸ì„ ë‚˜íƒ€ë‚´ì§€ëŠ” ì•ŠìŒ

$$\frac{TP+TN}{TP+FP+FN+TN}$$

- ì •ë°€ë„: ì–‘ì„± ì˜ˆì¸¡ì˜ ì •í™•ë„. ìŒì„±ì„ ì–‘ì„±ìœ¼ë¡œ ì˜¤íŒí•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš° ì‚¬ìš©

$$\frac{TP}{TP+FP}$$

- ì¬í˜„ìœ¨: ì‹¤ì œ ì–‘ì„± ê°’ì˜ ì •í™•ë„. ì–‘ì„±ì„ ìŒì„±ìœ¼ë¡œ ì˜¤íŒí•˜ë©´ ë¬¸ì œê°€ ë°œìƒí•˜ëŠ” ê²½ìš° ì‚¬ìš©

$$\frac{TP}{TP+FP}$$

- F1score: ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì˜ ì¡°í™”í‰ê· . ì •ë°€ë„ì™€ ì¬í˜„ìœ¨ì¤‘ ì–´ëŠ í•˜ë‚˜ì— í¸ì¤‘í•˜ì§€ ì•Šì€ ì ì ˆ ì¡°í•©ì— ì‚¬ìš©

$$2 * \frac{precision * recall}{precision + recall}$$

~~~python
# %%
from sklearn.metrics import confusion_matrix, classification_report
confustion_matrix(y_train, pred_train)
# ì´í•˜ ì¶œë ¥
# array([[  ì°¸ìŒì„±,  ê±°ì§“ì–‘ì„±],
#        [ ê±°ì§“ìŒì„±, ì°¸ì–‘ì„±]])
# array([[ 458812,  2],
#        [ 17345, 10]])

# %%
print(classification_report(y_train, pred_train))
# ì´í•˜ ì¶œë ¥
#               precision    recall  f1-score   support
#            0 -> ìŒì„±ê¸°ì¤€  
#            1 -> ì–‘ì„±ê¸°ì¤€
#     accuracy -> ì •í™•ë„    
#    macro avg -> ë‹¨ìˆœí‰ê· 
# weighted avg -> ìƒ˜í”Œì— ìˆ«ì ê°€ì¤‘ì¹˜ ì ìš©ëœ í‰ê·      
~~~

### 5.2. ë¡œê·¸ì†ì‹¤

$$logloss = -\frac{1}{N}\sum_{i=1}^{N}(y_{i}log(\hat{y}_i)+(1-y_{i})log(1-\hat{y}_i))$$

yëŠ” ì‹¤ì œí™•ë¥ , $\hat{y}$ëŠ” íƒ€ê¹ƒê°’ì¼ ì˜ˆì¸¡í™•ë¥ 

### 5.3. í”¼ë“œë°±ì„ ìœ„í•œ ê²°ê³¼ í•´ì„

ì˜ˆì¸¡ ì–‘ì„±ì€ ì „ì²´ ì¼€ì´ìŠ¤ 476169ëª… ì¤‘ 12ëª…ë§Œì´ ë³´í—˜ê¸ˆì„ ë°›ì„ê²ƒì´ë¼ê³  ì˜ˆì¸¡í•œë‹¤.
ì´ë•Œ ì •í™•ë„ëŠ” 96%ë¡œ ë†’ê²Œ ë‚˜ì™”ì§€ë§Œ, ì´ê²ƒì´ ì¢‹ì€ ëª¨ë¸ì¸ì§€ëŠ” ì•Œ ìˆ˜ ì—†ë‹¤.

ì´ë¥¼ í•´ì„í•˜ê¸° ìœ„í•´ ë³´í—˜ê¸ˆì„ ë°›ëŠ” ì¼€ì´ìŠ¤ë¥¼ ë¶„ì„í•œë‹¤

~~~python
# %%
proba=LGBMmodel.predict_proba(X_train)[:,1]

#ë³´í—˜ê¸ˆì„ íƒˆ í™•ë¥  ê·¸ë˜í”„
# %%
sns.distplot(proba)

#í™•ë¥ ë¶„í¬ì—ì„œ 0.5ë¥¼ ê¸°ì¤€ìœ¼ë¡œ 0.5ë³´ë‹¤ í¬ë©´ 1, ì‘ìœ¼ë©´ 0ìœ¼ë¡œ ì˜ˆì¸¡
# %%
len(proba[proba>0.5])
# ì¶œë ¥:12
~~~

### 5.4. ROC-Curveì™€ AUC

ROCê³¡ì„ ì˜ AUCëŠ” ìœ„ì™€ ê°™ì€ ìŒì„± í¸ì¤‘ì„ í•´ê²°í•˜ê¸°ìœ„í•œ í‰ê°€ ì§€í‘œë¡œ ì‚¬ìš©ëœë‹¤.

- ROCëŠ” ì°¸ì–‘ì„±ë¹„ì— ëŒ€í•œ ê±°ì§“ì–‘ì„±ë¹„ìœ¨ì„ ë‚˜íƒ€ë‚¸ ê³¡ì„ ì´ë©° AUCëŠ” ê·¸ ì•„ë˜ë©´ì ì´ë‹¤
- ì°¸ì–‘ì„±ë¥ (TPR) = ì‹¤ì œì–‘ì„±ì˜ ì •í™•ë„ = ì¬í˜„ë¥  = ë¯¼ê°ë„(sensitivity)=$\frac{TP}{TP+FN}$
- ê±°ì§“ì–‘ì„±ë¥ (FPR)= 1 - ì‹¤ì œìŒì„±ì˜ ì •í™•ë„ = 1 - íŠ¹ì´ë„(specificity)=$\frac{FP}{FP+TN}=1-\frac{TN}{FP+TN}$
- ex)  train ëŒ€ìƒ

~~~python
from sklearn.metrics import roc_curve, roc_auc_score
fig = plt.figure(figsize=(10,10))
fpr, tpr, thresholds = roc_curve(y_train, proba, pos_label=1)

roc_auc = auc(fpr, tpr)
plt.plot(fpr, tpr, label='(AUC = {0:0.2f})'.format(roc_auc))

plt.plot([0,1],[0,1],'k--')
plt.plot([0,0,1],[0,1,1],'g--')

plt.xlim([-0.05,1.05])
plt.ylim([-0.05,1.05])
plt.xlabel('1-Specificity')
plt.ylabel('Sensitivity')

plt.legend(loc="lower right")
plt.show()
~~~

### 5.5. ê°€ì¤‘ì¹˜ ë¶€ì—¬

ì–‘ì„± ì˜ˆì¸¡ì´ ê°€ì¤‘ì¹˜ë¥¼ ê°€ì§€ë„ë¡ êµ¬í˜„í•œë‹¤.

~~~python
# %%
LGBMmodel=LGBMClassifier(max_depth=grid.best_params_['max_depth'], random_state=grid.best_params_['random_state'],
scale_pos_weight=20)

LGBMmodel.fit(X_train, y_train)

pred_train = LGBMmodel.predict(X_train)
pred_test = LGBMmodel.predict(X_test)

# %%
confustion_matrix(y_train, pred_train)

# %%
print(classification_report(y_train, pred_train))
~~~
